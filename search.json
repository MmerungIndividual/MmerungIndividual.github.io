[{"title":"spark（爬老师）","url":"/2020/07/02/spark（爬老师）/","content":"# Spark简介\n## Java 8 新特性简介\n速度更快 \n修改底层数据结构：如HashMap（数组-链表-红黑树），HashSet，ConcurrentHashMap（CAS算法）\n修改垃圾回收机制：取消堆中的永久区（PremGen）->回收条件苛刻，使用元空间（MetaSpace）->直接使用物理内存->加载类文件\n代码更少（增加了新的语法Lambda表达式）\n强大的Stream API\n便于并行\n最大化减少空指针异常 Optional容器类\n## Spark RDD运算\nRDD（Resilient Distributed Datasets)弹性分布式数据集：\n是一个容错的、并行的数据结构，可以让用户显式地将数据存储到磁盘和内存中，并能控制数据的分区。同时，RDD还提供了一组丰富的操作来操作这些数据。在这些操作中，诸如map、flatMap、filter等转换操作实现了monad模式，很好地契合了Scala的集合操作。除此之外，RDD还提供了诸如join、groupBy、reduceByKey等更为方便的操作（注意，reduceByKey是action，而非transformation），以支持常见的数据运算。\n[爬老师本章参考ppt1](/_posts/spark（爬老师）/01Spark.ppt)\n[爬老师本章参考ppt2](/_posts/spark（爬老师）/Spark2.pptx)\n# 爬老师的重要概念分析\n## HashPartitioner原理简介\nHashPartitioner采用哈希的方式对<Key，Value>键值对数据进行分区。其数据分区规则为 partitionId = Key.hashCode % numPartitions，其中partitionId代表该Key对应的键值对数据应当分配到的Partition标识，Key.hashCode表示该Key的哈希值，numPartitions表示包含的Partition个数。\n## 累加器和广播\n### 二者简介\n在 Spark 中，提供了两种类型的全局变量：累加器 (accumulator) 与广播变量 (broadcast variable)：\n累加器：用来对信息进行聚合，主要用于累计计数等场景；\n广播变量：主要用于在节点间高效分发大对象。\n### 二者区别\nAccumulator每个 Task 任务的闭包都会持有自由变量的副本，如果变量很大且 Task 任务很多的情况下，这必然会对网络 IO 造成压力，为了解决这个情况，Spark 提供了广播变量。\n广播变量的做法很简单：就是不把副本变量分发到每个 Task 中，而是将其分发到每个 Executor，Executor 中的所有 Task 共享一个副本变量。\n## 缓存和持久化\n和RDD相似，DStreams也允许开发者持久化流数据到内存中。在DStream上使用persist()方法可以自动地持久化DStream中的RDD到内存中。如果DStream中的数据需要计算多次，这是非常有用的。像reduceByWindow和reduceByKeyAndWindow这种窗口操作、updateStateByKey这种基于状态的操作，持久化是默认的，不需要开发者调用persist()方法。\n例如通过网络（如kafka，flume等）获取的输入数据流，默认的持久化策略是复制数据到两个不同的节点以容错。\n注意，与RDD不同的是，DStreams默认持久化级别是存储序列化数据到内存中，\n## 参考资料\n[分区原理参考](https://blog.csdn.net/dmy1115143060/article/details/82620715)\n[RDD参考(包括运行机制等)](https://github.com/heibaiying/BigData-Notes/blob/master/notes/Spark_RDD.md)\n[SparkStreaming操作](https://github.com/heibaiying/BigData-Notes/blob/master/notes/Spark_Streaming%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C.md)\n[滑动窗口参考](https://blog.csdn.net/wisgood/article/details/55189656)\n[Spark常用问题参考](https://github.com/CheckChe0803/BigData-Interview#%E4%B8%89spark)\n[Spark官方文档](http://spark.apache.org/docs/latest/rdd-programming-guide.html#rdd-programming-guide)\n[Spark中文版文档整理](https://github.com/aiyanbo/spark-programming-guide-zh-cn)\n# 爬老师的代码\n## SWC.java\n词频统计\n```java\nimport org.apache.spark.SparkConf;\nimport org.apache.spark.api.java.JavaPairRDD;\nimport org.apache.spark.api.java.JavaRDD;\nimport org.apache.spark.api.java.JavaSparkContext;\nimport org.apache.spark.api.java.function.FlatMapFunction;\nimport org.apache.spark.api.java.function.Function2;\nimport org.apache.spark.api.java.function.PairFunction;\nimport scala.Tuple2;\n\nimport java.util.Arrays;\nimport java.util.Iterator;\nimport java.util.Scanner;\n\npublic class SWC {\n    public static void main(String[] args) {\n        String srcPath = null;\n        String desPath = null;\n        SparkConf sparkConf = new SparkConf().setMaster(\"local[3]\").setAppName(\"wordcount\");\n\n        JavaSparkContext ctx = new JavaSparkContext(sparkConf);\n        final JavaRDD<String> lines = ctx.textFile(\"words.txt\",3);\n        JavaRDD<String> words = lines.\n                flatMap(line -> Arrays.asList(line.split(\" \")).iterator());\n        JavaPairRDD<String, Integer> ones = words.mapToPair(word -> {\n            System.out.println(\"当前线程：\" + Thread.currentThread().getId());\n            return new Tuple2<String, Integer>(word, 1);\n        });//(hello,1) (word,1)\n        JavaPairRDD<String, Integer> result = ones.reduceByKey((a, b) -> {\n            System.out.println(\"当前线程：\" + Thread.currentThread().getId());\n            return a + b;\n        });\n        result.foreach(x -> System.out.println(x));\n\n        Scanner sc = new Scanner(System.in);\n        sc.next();\n        ctx.stop();\n    }\n}\n```\n## SortByKeyApi.java\n排序\n```java\npackage apr29;\n\nimport java.io.Serializable;\nimport java.util.ArrayList;\nimport java.util.Arrays;\nimport java.util.Comparator;\nimport java.util.List;\n\nimport org.apache.spark.HashPartitioner;\nimport org.apache.spark.SparkConf;\nimport org.apache.spark.api.java.JavaPairRDD;\nimport org.apache.spark.api.java.JavaRDD;\nimport org.apache.spark.api.java.JavaSparkContext;\nimport org.apache.spark.api.java.function.Function;\nimport org.apache.spark.api.java.function.Function2;\nimport org.apache.spark.api.java.function.PairFunction;\nimport org.apache.spark.storage.StorageLevel;\n\nimport scala.Tuple2;\n\npublic class SortByKeyApi {\n    public static void main(String[] xx){\n    \tSparkConf conf = new SparkConf();\n    \tconf.setMaster(\"local\");\n    \tconf.setAppName(\"WordCounter\");\n    \tconf.set(\"spark.default.parallelism\", \"4\");\n    \tJavaSparkContext ctx = new JavaSparkContext(conf);\n    \tctx.setLogLevel(\"ERROR\");\n    \t//创建RDD：1）通过读取外部存储 ----- 集群环境使用 2）通过内存中的集合\n    \tList<Tuple2<String, Integer>> data = new ArrayList<Tuple2<String, Integer>>();\n    \tdata.add(new Tuple2<>(\"Cake\", 2));\n    \tdata.add(new Tuple2<>(\"Bread\", 3));\n    \tdata.add(new Tuple2<>(\"Cheese\", 4));\n    \tdata.add(new Tuple2<>(\"Milk\", 1));\n    \tdata.add(new Tuple2<>(\"Toast\", 2));\n    \tdata.add(new Tuple2<>(\"Bread\", 2));\n    \tdata.add(new Tuple2<>(\"Egg\", 6));\n\n    \tJavaRDD<Tuple2<String, Integer>> rdd1 = ctx.parallelize(data, 2);\n\n        JavaPairRDD<String, Integer> mapRdd = rdd1.mapToPair(new PairFunction<Tuple2<String, Integer>, String, Integer>() {  \n\t        @Override\n\t        public Tuple2<String, Integer> call(Tuple2<String, Integer> t) throws Exception {  \n\t            return t;\n\t        }\n        }).partitionBy(new HashPartitioner(2)).persist(StorageLevel.MEMORY_ONLY());\n\n       //mapRdd.sortByKey(false).foreach(x -> System.out.println(x));\n       mapRdd.sortByKey(new MyComparator()).foreach(x->System.out.println(x));\n       ctx.stop();\n    }\n}\n\nclass MyComparator implements Comparator<String>, Serializable{\n\t  @Override\n\t  public int compare(String o1, String o2) {\n\t\t  return o2.compareTo(o1);\n\t  }\t\n}\n```\n## PersistDemo.java\n持久化\n```java\npackage may13;\nimport java.util.Iterator;\nimport org.apache.spark.SparkConf;  \nimport org.apache.spark.api.java.JavaPairRDD;  \nimport org.apache.spark.api.java.JavaRDD;  \nimport org.apache.spark.api.java.JavaSparkContext;  \nimport org.apache.spark.api.java.function.FlatMapFunction;  \nimport org.apache.spark.api.java.function.Function2;  \nimport org.apache.spark.api.java.function.PairFunction;  \nimport org.apache.spark.storage.StorageLevel;\nimport scala.Tuple2;  \nimport java.util.Arrays;  \nimport java.util.Iterator;\nimport java.util.Scanner;\npublic class PersisitDemo {\n    public static void main(String[] args) {\n        SparkConf sparkConf = new SparkConf().setMaster(\"local\").setAppName(\"wordcount\");\n        JavaSparkContext ctx = new JavaSparkContext(sparkConf);\n        ctx.setLogLevel(\"ERROR\");\n        //ctx.setCheckpointDir(\"file:///d:/checkpoint\");\n        final JavaRDD<String> lines = ctx.textFile(\"words.txt\");\n\n        JavaRDD<String> words = lines.flatMap(new FlatMapFunction<String, String>() {\n            @Override\n            public Iterator<String> call(String s) throws Exception {\n            \tSystem.out.println(\"flatMap...\");\n                return Arrays.asList(s.split(\" \")).iterator();\n            }\n        });\n\t\t//words.persist(StorageLevel.MEMORY_ONLY());\n        //.cache();\n\t\tJavaPairRDD<String, Integer> ones = words.mapToPair(new PairFunction<String, String, Integer>() {\n\t\t\t@Override\n\t\t\tpublic Tuple2<String, Integer> call(String s) throws Exception {\n\t\t\t\tSystem.out.println(\"mapToPair...\");\n\t\t\t\treturn new Tuple2<String, Integer>(s, 1);\n\t\t\t}\n\t\t});\n\t\tJavaPairRDD<String, Integer> counts = ones.reduceByKey(new Function2<Integer, Integer, Integer>() {\n\t\t\t@Override\n\t\t\tpublic Integer call(Integer integer, Integer integer2) throws Exception {\n\t\t\t\tSystem.out.println(\"reduceByKey...\");\n\t\t\t\treturn integer + integer2;\n\t\t\t}\n\t\t}).cache();\n\n        while(true){\n        \tScanner sc = new Scanner(System.in);\n        \tString line = sc.next();\n        \tif(line.equals(\"END\")){\n        \t\tbreak;\n        \t}\n\n\t\t\tcounts = counts.unpersist();\n\t        //counts.saveAsTextFile(args[1]);\n\t        counts.foreach(x -> System.out.println(x));\n\n        }\n        ctx.stop();\n    }  \n}\n```\n## Checkpointdemo.java\n持久化\n```java\npackage May20;\n\nimport org.apache.spark.SparkConf;\nimport org.apache.spark.api.java.JavaPairRDD;\nimport org.apache.spark.api.java.JavaRDD;\nimport org.apache.spark.api.java.JavaSparkContext;\nimport org.apache.spark.api.java.function.FlatMapFunction;\nimport org.apache.spark.api.java.function.Function2;\nimport org.apache.spark.api.java.function.PairFunction;\nimport org.apache.spark.storage.StorageLevel;\nimport scala.Tuple2;\n\nimport java.util.Arrays;\nimport java.util.Iterator;\n\n\npublic class CheckpointDemo {\n    public static void main(String[] args) {\n        SparkConf sparkConf = new SparkConf().setMaster(\"local\").setAppName(\"wordcount\");\n        JavaSparkContext ctx = new JavaSparkContext(sparkConf);\n        ctx.setLogLevel(\"ERROR\");\n        ctx.setCheckpointDir(\"file:///d:/checkpoint\");\n        final JavaRDD<String> lines = ctx.textFile(\"words.txt\");\n\n        JavaRDD<String> words = lines.flatMap(new FlatMapFunction<String, String>() {\n            @Override\n            public Iterator<String> call(String s) throws Exception {\n            \tSystem.out.println(\"flatMap... s \" + s);\n                return Arrays.asList(s.split(\" \")).iterator();\n            }\n        });\n        JavaPairRDD<String, Integer> ones = words.mapToPair(new PairFunction<String, String, Integer>() {  \n            @Override\n            public Tuple2<String, Integer> call(String s) throws Exception {  \n            \tSystem.out.println(\"mapToPair... s \" + s);\n                return new Tuple2<String, Integer>(s, 1);\n            }\n        });\n        ones.persist(StorageLevel.DISK_ONLY());\n        ones.checkpoint(); //检查点就是把rdd计算结果存储到HDFS等高可靠的文件系统上去，但是它的执行时间并非在当前这个JOB完成，而是在当前JOB完成后，框架会新启一个JOB来专门处理检查点。\n        \n        JavaPairRDD<String, Integer> counts = ones.reduceByKey(new Function2<Integer, Integer, Integer>() {  \n            @Override\n            public Integer call(Integer integer, Integer integer2) throws Exception {  \n            \tSystem.out.println(\"reduceByKey...\");\n                return integer + integer2;\n            }\n        });\n        System.out.println(\"before action:\" + counts.toDebugString());\n        //counts.saveAsTextFile(args[1]);\n        counts.foreach(x -> System.out.println(x));\n        System.out.println(\"after action:\" + counts.toDebugString());       \n        ones.unpersist();       \n        ctx.stop();\n        ctx.close();\n    }\n}\n```\n## SparkStreamWC.java\n流式计算词频统计\n```java\npackage may27;\n\nimport org.apache.spark.SparkConf;\nimport org.apache.spark.api.java.JavaPairRDD;  \nimport org.apache.spark.api.java.JavaRDD;  \nimport org.apache.spark.api.java.JavaSparkContext;  \nimport org.apache.spark.api.java.function.FlatMapFunction;  \nimport org.apache.spark.api.java.function.Function2;  \nimport org.apache.spark.api.java.function.PairFunction;  \nimport org.apache.spark.streaming.Duration;\nimport org.apache.spark.streaming.api.java.JavaDStream;\nimport org.apache.spark.streaming.api.java.JavaPairDStream;\nimport org.apache.spark.streaming.api.java.JavaStreamingContext;\n\nimport scala.Tuple2;  \n  \n\nimport java.util.Arrays;  \nimport java.util.Iterator;\n\npublic class SparkStreamWC {\n\t   public static void main(String[] args) {\n\t        SparkConf conf = new SparkConf().setMaster(\"local[4]\").setAppName(\"wordcount\"); \n\t        JavaStreamingContext jssc = new JavaStreamingContext(conf, new Duration(3000));\n\t        JavaDStream<String> lines = jssc.socketTextStream(\"192.168.92.1\", 8888);\n\t        //JavaSparkContext ctx = new JavaSparkContext(conf);\n\t        JavaDStream<String> words = lines.flatMap(new FlatMapFunction<String, String>() {\n\t            @Override\n\t            public Iterator<String> call(String s) throws Exception {\n\t            \tSystem.out.println(s);\n\t                return Arrays.asList(s.split(\" \")).iterator();\n\t            }\n\t        });\n\n\t        JavaPairDStream<String, Integer> ones = words.mapToPair(new PairFunction<String, String, Integer>() {  \n\t            @Override  \n\t            public Tuple2<String, Integer> call(String s) throws Exception {  \n\t                return new Tuple2<String, Integer>(s, 1);  \n\t            }  \n\t        });  \n\n\t        JavaPairDStream<String, Integer> counts = ones.reduceByKey(new Function2<Integer, Integer, Integer>() {  \n\t            @Override  \n\t            public Integer call(Integer integer, Integer integer2) throws Exception {  \n\t                return integer + integer2;  \n\t            }  \n\t        });  \n\n\t        counts.print();\n\n\t        jssc.start();\n\t        try {\n\t\t\t\tjssc.awaitTermination();\n\t\t\t} catch (InterruptedException e) {\n\t\t\t\te.printStackTrace();\n\t\t\t} \n\t    }      \n}\n```\n## WC_Stream_Window\n滑动窗口\n```java\nimport org.apache.spark.SparkConf;\nimport org.apache.spark.api.java.JavaPairRDD;  \nimport org.apache.spark.api.java.JavaRDD;  \nimport org.apache.spark.api.java.JavaSparkContext;  \nimport org.apache.spark.api.java.function.FlatMapFunction;  \nimport org.apache.spark.api.java.function.Function2;  \nimport org.apache.spark.api.java.function.PairFunction;  \nimport org.apache.spark.streaming.Duration;\nimport org.apache.spark.streaming.api.java.JavaDStream;\nimport org.apache.spark.streaming.api.java.JavaPairDStream;\nimport org.apache.spark.streaming.api.java.JavaStreamingContext;\n\nimport scala.Tuple2;  \n  \n\nimport java.util.Arrays;  \nimport java.util.Iterator;\n\npublic class WC_Stream_Window {\n\t   public static void main(String[] args) {\n\t        SparkConf conf = new SparkConf().setMaster(\"local[4]\").setAppName(\"wordcount\");\n\t        //设置批次间隔为1秒---，也就是说每隔1秒就生产1个RDD\n\t        JavaStreamingContext jssc = new JavaStreamingContext(conf, new Duration(1000));\n\t        jssc.sparkContext().setLogLevel(\"WARN\");\n\t        jssc.checkpoint(\"file:///d:/checkpoint\");\n\t        JavaDStream<String> lines = jssc.socketTextStream(\"localhost\", 8888);\n\t        //窗口长度6秒   窗口滑动频率3秒\n\t        //lines = lines.window(new Duration(6000), new Duration(2000));\n\n\t        JavaDStream<String> words = lines.flatMap(new FlatMapFunction<String, String>() {\n\t            @Override\n\t            public Iterator<String> call(String s) throws Exception {\n\t            \t//System.out.println(\"flatMap：\" + s);\n\t                return Arrays.asList(s.split(\" \")).iterator();\n\t            }\n\t        });\n\t        JavaPairDStream<String, Integer> ones = words.mapToPair(s->new Tuple2<String, Integer>(s, 1));  \n\n\t        //在整个窗口上执行规约\n//\t        JavaPairDStream<String, Integer> counts = ones.reduceByKeyAndWindow(\n//\t        \t\t                          (x, y) ->{\n//\t        \t\t                        \t  System.out.println(\"规约数据。。。\");\n//\t        \t                                  return x + y;\n//\t        \t                              },\n//\t        \t\t                          new Duration(4000), new Duration(2000));\n\n\t        //只考虑新进入窗口的数据，和离开窗口的数据，增量计算规约结果，该操作必须开启checkpoint\n\t        JavaPairDStream<String, Integer> counts = ones.reduceByKeyAndWindow(\n\t\t\t\t\t\t\t\t\t\t\t        \t\t(x, y) -> x + y,\n\t\t\t\t\t\t\t\t\t\t\t        \t\t(x, y) -> x - y,\n\t\t\t\t\t\t\t\t\t\t\t        \t\tnew Duration(6000),\n\t\t\t\t\t\t\t\t\t\t\t        \t\tnew Duration(2000));\n\n//\t        JavaPairDStream<String, Integer> counts = ones.reduceByKey(\n//\t\t\t\t   (x, y)->{\n//\t\t\t\t\t   System.out.println(\"规约数据： x= \" + x + \" y= \" + y);\n//\t\t\t\t\t   return x + y;\n//\t\t\t\t   }\n//\t\t    );\n\t        counts.print();\n\n\t        jssc.start();\n\t        try {\n\t\t\t\tjssc.awaitTermination();\n\t\t\t} catch (InterruptedException e) {\n\t\t\t\te.printStackTrace();\n\t\t\t}\n\t        jssc.close();\n\t    }\n}\n\n```\n\n","tags":["答案"]},{"title":"机器学习（爬老师）","url":"/2020/06/30/机器学习（爬老师）/","content":"# 概述\n## 什么是监督机器学习和非监督机器学习？\n**监督学习**中，提供给算法的包含所需解决方案的训练数据，成为标签或标记。\n常见的监督学习算法:\nK近邻算法\n线性回归\nlogistic回归\n支持向量机（SVM）\n决策树和随机森林\n神经网络\n**无监督学习**的训练数据都是未经标记的，算法会在没有指导的情况下自动学习。\n常见的非监督学习算法:\n1.聚类算法类\n    K均值算法（K-means）\n    基于密度的聚类方法(DBSCAN)\n    最大期望算法\n2.可视化和降维类\n    主成分分析\n    核主成分分析\n# 梯度下降算法和线性回归\n## 概述\n梯度下降（gradient descent）在机器学习中应用十分的广泛，不论是在线性回归还是Logistic回归中，它的主要目的是通过迭代找到目标函数的最小。\n## 单变量函数的梯度下降\n我们假设有一个单变量的函数\n$J(\\Theta)=\\theta^2$\n函数的微分，直接求导就可以得到\n$\\dot{J(\\theta)}$\n初始化，也就是起点，起点可以随意的设置，这里设置为1\n$\\theta_0=1$\n学习率也可以随意的设置，这里设置为0.4\n$\\alpha=0.4$\n梯度下降的计算公式\n$\\theta_1 = \\theta_0-\\alpha*\\dot{J(\\theta_0)}$\n## 多变量函数的梯度下降\n我们假设有一个目标函数\n$J(\\Theta)=\\theta_1^2+\\theta_2^2$\n初始的学习率为：\n$\\alpha=0.1$\n函数的梯度为：\n$\\nabla J(\\Theta)=<2\\theta_1,2\\theta_2>$\n根据公式进行多次迭代:\n{%img /img/diedai.png 200 480%}\n## 线性回归代码（LinearRegression.py）\n其中写了三种梯度下降的方法，\n随机梯度下降\n批量梯度下降\n小批量梯度下降\n```bash\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas\n\n#y = weight * x + bias  建模\n\ndef getData():\n    x = []\n    y = []\n    with open('data.csv', 'r', encoding='UTF-8') as f:\n        lines = f.readlines()\n    f.close()\n    for line in lines:\n        x.append(float(line.split(\",\")[0]))\n        y.append(float(line.split(\",\")[1]))\n    return x, y\n\ndef genLineDot(xielv,jieju):\n    x = np.linspace(-3,3,100)\n    #print(\"x_train.shape == \", x.shape)\n    y = xielv * x + jieju\n    return x, y\n\n#解析解：有缺陷：在数据量特别大情况下，计算量大；并非所有情况下都有解（矩阵不可逆）\ndef fitSLR(x, y):\n    numerator = 0\n    denominator = 0\n    for index in range(0, len(x)):\n        numerator += (x[index] - np.mean(x)) * (y[index] - np.mean(y))\n        # denominator+=(x[index] - average_X)*(x[index] - average_X)\n        denominator += np.square(x[index] - np.mean(x))\n    weight = numerator / float(denominator)    #斜率\n    bias = np.mean(y) - weight * np.mean(x)    #截距\n    return bias, weight\n\n#批量梯度下降\ndef fitSLR2(x, y):\n    count = 0\n    threshold = 0.00001\n    b = 0.0 # intial b\n    w = 0.0 # intial w\n    lr = 0.001 # learning rate\n    iteration = 500\n    #for i in range(iteration):\n    while(True):\n        count = count + 1;\n        b_grad = 0.0  #对b的偏导数\n        w_grad = 0.0  #对w的偏导数\n        for n in range(len(x)):\n            w_grad = w_grad - 2.0 * (y[n] - b - w * x[n]) * x[n]   #对w的偏微分\n            b_grad = b_grad - 2.0 * (y[n] - b - w * x[n]) * 1.0    #对b的偏微分\n        b = b - lr * b_grad  #梯度下降法来迭代b参数值\n        w = w - lr * w_grad   #梯度下降法来迭代w参数值\n        if(abs(lr*b_grad / b) < threshold and abs(lr*w_grad / w) < threshold):\n            break;\n\n        print(\"第\"+str(count)+\"次迭代完成：\");\n        print(\"b==\",b);\n        print(\"w==\",w);\n    return b,w\n\n#随机梯度下降\ndef fitSLR3(x, y):\n    threshold = 0.0001\n    b = 0.0 # intial b\n    w = 0.0 # intial w\n\n    iteration = 30000\n    for n in range(len(x)):\n        b_grad = 0.0\n        w_grad = 0.0\n        lr = 0.0001 # learning rate        \n        for i in range(iteration):\n            w_grad = w_grad - 2.0 * (y[n] - b - w * x[n]) * x[n]\n            b_grad = b_grad - 2.0 * (y[n] - b - w * x[n]) * 1.0\n            b = b - lr*b_grad\n            w = w - lr*w_grad\n            lr = lr * 0.90 #动态调整学习率\n        print(\"第\"+str(n)+\"个样本\" + str(iteration) + \"次迭代完成：\");\n        print(\"b==\",b);\n        print(\"w==\",w);\n    return b,w\n\n#小批量梯度下降\ndef fitSLR4(x, y):\n    batch = int(len(x) / 10) #10\n    start = int(0)\n    end = batch\n    threshold = 0.00001\n    b = 0.0 # intial b\n    w = 0.0 # intial w\n    iteration = 100\n\n    while(True):\n        lr = 0.0001  # learning rate\n        b_grad = 0.0  #对b的偏导数\n        w_grad = 0.0  #对w的偏导数\n        for i in range(iteration):\n            for n in range(start,end):\n                w_grad = w_grad - 2.0 * (y[n] - b - w * x[n]) * x[n]   #对w的偏微分\n                b_grad = b_grad - 2.0 * (y[n] - b - w * x[n]) * 1.0    #对b的偏微分\n            b = b - lr * b_grad  #梯度下降法来迭代b参数值\n            w = w - lr * w_grad  #梯度下降法来迭代w参数值\n            lr = lr * 0.90  # 动态调整学习率\n            print(\"第\"+str(i+1)+\"次迭代完成：\")\n        print(\"b==\",b)\n        print(\"w==\",w)\n        start += batch\n        end += batch\n        if(end >= len(x) + batch):\n            break\n    return b,w\n    \ndef predict(x):\n    return weight * x + bias\n\nif __name__ == '__main__':\n    x_train, y_train = getData() \n    # x = [1,3,2,1,3]\n    # y = [14,24,18,17,27]\n    bias, weight = fitSLR4(x_train, y_train)\n    print(\"finally bias==\",bias)\n    print(\"finally weight==\",weight)\n    #predict_Y = predict(6)\n    #print(\"predict: \" + str(predict_Y))\n\n    compute_Y = []\n    x,y = genLineDot(weight, bias)\n    \n    plt.plot(x_train, y_train,\"+\")\n    plt.plot(x, y,\".\")\n    plt.show()\n```\n## 三种梯度下降比较\n**批量梯度下降法**是最原始的形式，它是指在每一次迭代时使用所有样本来进行梯度的更新。\n优点：\n  （1）一次迭代是对所有样本进行计算，此时利用矩阵进行操作，实现了并行。\n  （2）由全数据集确定的方向能够更好地代表样本总体，从而更准确地朝向极值所在的方向。当目标函数为凸函数时，BGD一定能够得到全局最优。\n  缺点：\n  （1）当样本数目 m 很大时，每迭代一步都需要对所有样本计算，训练过程会很慢。\n  从迭代的次数上来看，BGD迭代的次数相对较少。\n**随机梯度下降法**不同于批量梯度下降，随机梯度下降是每次迭代使用一个样本来对参数进行更新。使得训练速度加快。\n优点：\n  （1）由于不是在全部训练数据上的损失函数，而是在每轮迭代中，随机优化某一条训练数据上的损失函数，这样每一轮参数的更新速度大大加快。\n  缺点：\n  （1）准确度下降。由于即使在目标函数为强凸函数的情况下，SGD仍旧无法做到线性收敛。\n  （2）可能会收敛到局部最优，由于单个样本并不能代表全体样本的趋势。\n  （3）不易于并行实现。\n**小批量梯度下降**，是对批量梯度下降以及随机梯度下降的一个折中办法。其思想是：每次迭代 使用 batch_size 个样本来对参数进行更新。\n 优点：\n  （1）通过矩阵运算，每次在一个batch上优化神经网络参数并不会比单个数据慢太多。\n  （2）每次使用一个batch可以大大减小收敛所需要的迭代次数，同时可以使收敛到的结果更加接近梯度下降的效果。(比如上例中的30W，设置batch_size=100时，需要迭代3000次，远小于SGD的30W次)\n  （3）可实现并行化。\n  缺点：\n  （1）batch_size的不当选择可能会带来一些问题。\n# 逻辑回归和线性回归\n## 逻辑回归于线性回归的区别于联系\n**线性回归**试图学得一个线性模型尽可能准确预测实值输出标记。表达式为\n$f(x)=W^Tx+b$\n(1)逻辑回归和线性回归首先都是广义的线性回归。\n(2)经典线性模型的优化目标函数是最小二乘，而逻辑回归则是似然函数。\n(3)线性回归在整个实数域范围内进行预测，敏感度一致，而分类范围，需要在[0,1]。逻辑 回归就是一种减小预测范围，将预测值限定为[0,1]间的一种回归模型，因而对于这类问题来说，逻辑回归的鲁棒性比线性回归的要好。\n或者说，线性回归模型无法做到sigmoid的非线性形式，sigmoid可以轻松处理0/1分类问题。$g(z)=\\frac{1}{1+e^{-z}}$\n逻辑回归实际上是一种分类模型。\n**逻辑回归**将 分类任务 的真实标记 y 与 线性回归 模型的预测值联系起来。\n以一个二分类任务为例，其输出标记 $y\\in(0,1)$ ，而线性回归模型产生的预测值 $z=W^Tx+b$ $z\\in(-\\infin,+\\infin)$是连续值，于是，我们需要将连续值z转换为0/1值。\n## 逻辑回归代码(logitic.py)\n```bash\n## 逻辑回归\nfrom numpy import *\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom pylab import mpl\n\nmpl.rcParams['font.sans-serif']=['SimHei']\n# 解决x轴负号不正常显示\nmpl.rcParams['axes.unicode_minus']=False\n\n#Nx=2  # 假设两个特征\n#M = 100 # 100个样本\n#Num=5000\n\nX=[];Y=[]\n# 加载数据\ndef loadDataSet():\n    f=open('testSet.txt')\n    # 逐行读入数据  使用strip去掉头尾的空格  split根据空格分组\n    for line in f.readlines():\n        nline=line.strip().split()\n        # x 需要添加一列\n        X.append([1.0,float(nline[0]),float(nline[1])])\n        Y.append(int(nline[2]))\n    return X,Y\n\n# 定义sigmoid函数\ndef sigmoid(X):\n    return 1.0/(1 + exp(-X))\n\n# 绘制图片\ndef plotBestFit(X,Y,J,Weights):\n    '''\n    # 绘制代价函数曲线\n    fig1 = plt.figure(1)\n    plt.plot(J)\n    plt.title(u'代价函数随迭代次数的变化')\n    plt.xlabel(u'迭代次数')\n    plt.ylabel(u'代价函数的值')\n    '''\n    # 绘制样本数据\n    n = Y.shape[1]  # 样本数\n    # 根据训练样本标记不同，分为两类不同的点\n    xcord1=[]; ycord1=[]\n    xcord2=[]; ycord2=[]\n    for i in range(n):\n        if int(Y[0,i])==1:\n            xcord1.append(X[1,i])\n            ycord1.append(X[2,i])\n        else:\n            xcord2.append(X[1,i])\n            ycord2.append(X[2,i])\n    fig3 = plt.figure(3)\n    plt.scatter(xcord1,ycord1,c='b',marker='o')\n    plt.scatter(xcord2,ycord2,c='r',marker='s')\n\n    #绘制决策边界\n    x = linspace(-3,3,100).reshape(100,1) # 生成一个数组\n    print(\"Weights == \", Weights)\n    y = (-Weights[0,0]-Weights[1,0]*x)/Weights[2,0]\n    print(shape(x),shape(y))\n    plt.plot(x,y,c='y')\n    plt.title(u'逻辑分类结果示意图')\n    plt.xlabel(u'x')\n    plt.ylabel(u'y')\n    plt.show()\n\n\n# 梯度下降法\ndef GardDescent(X,Y,alpha=0.001,num=5000):\n    Losts = zeros((num,1))\n    # 只有2个特征\n    n = X.shape[0]  # 特征数\n    Weights = ones((n,1))\n    m = X.shape[1]  # 样本个数\n    for i in range(num):\n        Z = np.dot(Weights.T,X)\n        A = sigmoid(Z)\n        dW =- np.dot(X,(Y-A).T)  # 根据代价函数对w的偏导得出的\n        if(dW[0,0] < 0.01 and dW[1,0] < 0.01 and dW[2,0] < 0.01 ):\n            print(\"break!!!\")\n            print(i)\n            break\n        Weights = Weights - alpha * dW  # 迭代更新权重  梯度下降\n        Losts[i] = -1/m*(np.dot(Y, log(A.T)) + np.dot((1-Y),log((1 - A).T))) #记录每次迭代的损失值，便宜观测梯度下降的效果\n    return Losts,Weights\n\nif __name__ == '__main__':\n    X,Y=loadDataSet()\n    # 将列表转化为矩阵\n    X = mat(X).T\n    Y = mat(Y).reshape((100, 1)).T\n#    print(np.shape(X), np.shape(Y))\n    # 梯度下降求解最优参数\n    Losts,Weights = GardDescent(X,Y,0.001,50000)\n    # 绘图\n    plotBestFit(X,Y,Losts,Weights)\n```\n参考于:[参考地址1](https://zhuanlan.zhihu.com/p/53363181)\n[参考地址2](https://zhuanlan.zhihu.com/p/47200561)\n# 感知机和SVM\n## 概述\n**感知机**是一种线性分类模型。也就是说针对的是线性可分的数据。最常见的例子就是二维平面内的一堆数据可以通过一条直线分割开。\n则空间中任意一点$x_0$ 到超平面的距离可以表示为$\\frac{||w^Tx_0+b||}{||w||}$\n则整个学习过程中的损失函数可以如下定义，即考虑所有误分类点到超平面的距离并使其最小。\n{%img /img/keyou.png 80 120%}\n不考虑模问题转化为求解\n{%img /img/keyou2.png 80 120%}\n## 感知机代码(perception.py)\n```bash\nimport copy\nfrom matplotlib import pyplot as plt\nfrom matplotlib import animation\n\ntraining_set = [[(1, 2), 1], [(2, 3), 1], [(3, 1), -1], [(4, 2), -1]]  # 训练数据集\nw = [1, 1]  # 参数初始化\nb = 0\nhistory = []  # 用来记录每次更新过后的w,b\n\ndef update(item):\n    \"\"\"\n    随机梯度下降更新参数\n    :param item: 参数是分类错误的点\n    :return: nothing 无返回值\n    \"\"\"\n    global w, b, history  # 把w, b, history声明为全局变量\n    w[0] += 1 * item[1] * item[0][0]  # 根据误分类点更新参数,这里学习效率设为1\n    w[1] += 1 * item[1] * item[0][1]\n    b += 1 * item[1]\n    history.append([copy.copy(w), b])  # 将每次更新过后的w,b记录在history数组中\n\ndef cal(item):\n    \"\"\"\n    计算item到超平面的距离,输出yi(w*xi+b)\n    （我们要根据这个结果来判断一个点是否被分类错了。如果yi(w*xi+b)<0,则分类错了）\n    :param item:\n    :return:\n    \"\"\"\n    res = 0\n    for i in range(len(item[0])):  # 迭代item的每个坐标，对于本文数据则有两个坐标x1和x2\n        res += item[0][i] * w[i]\n    res += b\n    res *= item[1]  # 这里是乘以公式中的yi\n    return res\n\ndef check():\n    \"\"\"\n    检查超平面是否已将样本正确分类\n    :return: true如果已正确分类则返回True\n    \"\"\"\n    flag = False\n    for item in training_set:\n        if cal(item) <= 0:  # 如果有分类错误的\n            flag = True  # 将flag设为True\n            update(item)  # 用误分类点更新参数\n    if not flag:  # 如果没有分类错误的点了\n        print(\"最终结果: w: \" + str(w) + \"b: \" + str(b))  # 输出达到正确结果时参数的值\n    return flag  # 如果已正确分类则返回True,否则返回False\n\nif __name__ == \"__main__\":\n    for i in range(1000):  # 迭代1000遍\n        if not check(): break  # 如果已正确分类，则结束迭代\n    # 以下代码是将迭代过程可视化\n    # 首先建立我们想要做成动画的图像figure, 坐标轴axis,和plot element\n    fig = plt.figure()\n    ax = plt.axes(xlim=(0, 2), ylim=(-2, 2))\n    line, = ax.plot([], [], 'g', lw=2)  # 画一条线\n    label = ax.text([], [], '')\n\n    def init():\n        line.set_data([], [])\n        x, y, x_, y_ = [], [], [], []\n        for p in training_set:\n            if p[1] > 0:\n                x.append(p[0][0])  # 存放yi=1的点的x1坐标\n                y.append(p[0][1])  # 存放yi=1的点的x2坐标\n            else:\n                x_.append(p[0][0])  # 存放yi=-1的点的x1坐标\n                y_.append(p[0][1])  # 存放yi=-1的点的x2坐标\n        plt.plot(x, y, 'bo', x_, y_, 'rx')  # 在图里yi=1的点用点表示，yi=-1的点用叉表示\n        plt.axis([-6, 6, -6, 6])  # 横纵坐标上下限\n        plt.grid(True)  # 显示网格\n        plt.xlabel('x1')  # 这里我修改了原文表示\n        plt.ylabel('x2')  # 为了和原理中表达方式一致，横纵坐标应该是x1,x2\n        plt.title('Perceptron Algorithm ')  # 给图一个标题：感知机算法\n        return line, label\n\n    def animate(i):\n        global history, ax, line, label\n        w = history[i][0]\n        b = history[i][1]\n        if w[1] == 0: return line, label\n        # 因为图中坐标上下限为-6~6，所以我们在横坐标为-7和7的两个点之间画一条线就够了，这里代码中的xi,yi其实是原理中的x1,x2\n        x1 = -7\n        y1 = -(b + w[0] * x1) / w[1]\n        x2 = 7\n        y2 = -(b + w[0] * x2) / w[1]\n        line.set_data([x1, x2], [y1, y2])  # 设置线的两个点\n        x1 = 0\n        y1 = -(b + w[0] * x1) / w[1]\n        label.set_text(history[i])\n        label.set_position([x1, y1])\n        return line, label\n\n    print(\"参数w,b更新过程：\", history)\n    anim = animation.FuncAnimation(fig, animate, init_func=init, frames=len(history),\n                                   interval=1000, repeat=False, blit=True)\n    plt.show()\n\n```\n## SVM解释和作业截图\n**支持向量机**（support vector machines, SVM）是一种二分类模型，它的基本模型是定义在特征空间上的间隔最大的线性分类器，间隔最大使它有别于感知机；SVM还包括核技巧，这使它成为实质上的非线性分类器。SVM的的学习策略就是间隔最大化，可形式化为一个求解凸二次规划的问题，也等价于正则化的合页损失函数的最小化问题。SVM的的学习算法就是求解凸二次规划的最优化算法。\n**作业**根据课堂展示的理论推导，手动计算： 有两个正样本点（3，3）、（4，3）和一个负样本点（1，1），求出SVM的分割超平面。\n{%img /img/svm.png%}\n## SVM代码\n```bash\nfrom numpy import *\nimport re\n\n\ndef loadDataSet(filename): #读取数据\n    dataMat=[]\n    labelMat=[]\n    pattern = re.compile('\\s+')\n    fr=open(filename)\n    for line in fr.readlines():\n        line = re.sub(pattern, '_',line)\n        #print(\"line\",line)\n        lineArr=line.strip().split('_')\n        dataMat.append([float(lineArr[0]),float(lineArr[1])])\n        labelMat.append(float(lineArr[2]))\n    return dataMat,labelMat #返回数据特征和数据类别\n\ndef selectJrand(i,m): #在0-m中随机选择一个不是i的整数\n    j=i\n    while (j==i):\n        j=int(random.uniform(0,m))\n    return j\n\ndef clipAlpha(aj,H,L):  #保证a在L和H范围内（L <= a <= H）\n    if aj>H:\n        aj=H\n    if L>aj:\n        aj=L\n    return aj\n\ndef kernelTrans(X, A, kTup): #核函数，输入参数,X:支持向量的特征树；A：某一行特征数据；kTup：('lin',k1)核函数的类型和参数\n    m,n = shape(X)\n    K = mat(zeros((m,1)))\n    if kTup[0]=='lin': #线性函数\n        K = X * A.T\n    elif kTup[0]=='rbf': # 径向基函数(radial bias function)\n        for j in range(m):\n            deltaRow = X[j,:] - A\n            K[j] = deltaRow*deltaRow.T\n        K = exp(K/(-1*kTup[1]**2)) #返回生成的结果\n    else:\n        raise NameError('Houston We Have a Problem -- That Kernel is not recognized')\n    return K\n\n\n#定义类，方便存储数据\nclass optStruct:\n    def __init__(self,dataMatIn, classLabels, C, toler, kTup):  # 存储各类参数\n        self.X = dataMatIn  #数据特征\n        self.labelMat = classLabels #数据类别\n        self.C = C #软间隔参数C，参数越大，非线性拟合能力越强\n        self.tol = toler #停止阀值\n        self.m = shape(dataMatIn)[0] #数据行数\n        self.alphas = mat(zeros((self.m,1)))\n        self.b = 0 #初始设为0\n        self.eCache = mat(zeros((self.m,2))) #缓存\n        self.K = mat(zeros((self.m,self.m))) #核函数的计算结果\n        for i in range(self.m):\n            self.K[:,i] = kernelTrans(self.X, self.X[i,:], kTup)\n\n\ndef calcEk(oS, k): #计算Ek（参考《统计学习方法》p127公式7.105）\n    fXk = float(multiply(oS.alphas,oS.labelMat).T*oS.K[:,k] + oS.b)\n    Ek = fXk - float(oS.labelMat[k])\n    return Ek\n\n#随机选取aj，并返回其E值\ndef selectJ(i, oS, Ei):\n    maxK = -1\n    maxDeltaE = 0\n    Ej = 0\n    oS.eCache[i] = [1,Ei]\n    validEcacheList = nonzero(oS.eCache[:,0].A)[0]  #返回矩阵中的非零位置的行数\n    if (len(validEcacheList)) > 1:\n        for k in validEcacheList:\n            if k == i:\n                continue\n            Ek = calcEk(oS, k)\n            deltaE = abs(Ei - Ek)\n            if (deltaE > maxDeltaE): #返回步长最大的aj\n                maxK = k\n                maxDeltaE = deltaE\n                Ej = Ek\n        return maxK, Ej\n    else:\n        j = selectJrand(i, oS.m)\n        Ej = calcEk(oS, j)\n    return j, Ej\n\n\ndef updateEk(oS, k): #更新os数据\n    Ek = calcEk(oS, k)\n    oS.eCache[k] = [1,Ek]\n\n#首先检验ai是否满足KKT条件，如果不满足，随机选择aj进行优化，更新ai,aj,b值\ndef innerL(i, oS): #输入参数i和所有参数数据\n    Ei = calcEk(oS, i) #计算E值\n    if ((oS.labelMat[i]*Ei < -oS.tol) and (oS.alphas[i] < oS.C)) or ((oS.labelMat[i]*Ei > oS.tol) and (oS.alphas[i] > 0)): #检验这行数据是否符合KKT条件 参考《统计学习方法》p128公式7.111-113\n        j,Ej = selectJ(i, oS, Ei) #随机选取aj，并返回其E值\n        alphaIold = oS.alphas[i].copy()\n        alphaJold = oS.alphas[j].copy()\n        if (oS.labelMat[i] != oS.labelMat[j]): #以下代码的公式参考《统计学习方法》p126\n            L = max(0, oS.alphas[j] - oS.alphas[i])\n            H = min(oS.C, oS.C + oS.alphas[j] - oS.alphas[i])\n        else:\n            L = max(0, oS.alphas[j] + oS.alphas[i] - oS.C)\n            H = min(oS.C, oS.alphas[j] + oS.alphas[i])\n        if L==H:\n            print(\"L==H\")\n            return 0\n        eta = 2.0 * oS.K[i,j] - oS.K[i,i] - oS.K[j,j] #参考《统计学习方法》p127公式7.107\n        if eta >= 0:\n            print(\"eta>=0\")\n            return 0\n        oS.alphas[j] -= oS.labelMat[j]*(Ei - Ej)/eta #参考《统计学习方法》p127公式7.106\n        oS.alphas[j] = clipAlpha(oS.alphas[j],H,L) #参考《统计学习方法》p127公式7.108\n        updateEk(oS, j)\n        if (abs(oS.alphas[j] - alphaJold) < oS.tol): #alpha变化大小阀值（自己设定）\n            print(\"j not moving enough\")\n            return 0\n        oS.alphas[i] += oS.labelMat[j]*oS.labelMat[i]*(alphaJold - oS.alphas[j])#参考《统计学习方法》p127公式7.109\n        updateEk(oS, i) #更新数据\n        #以下求解b的过程，参考《统计学习方法》p129公式7.114-7.116\n        b1 = oS.b - Ei- oS.labelMat[i]*(oS.alphas[i]-alphaIold)*oS.K[i,i] - oS.labelMat[j]*(oS.alphas[j]-alphaJold)*oS.K[i,j]\n        b2 = oS.b - Ej- oS.labelMat[i]*(oS.alphas[i]-alphaIold)*oS.K[i,j]- oS.labelMat[j]*(oS.alphas[j]-alphaJold)*oS.K[j,j]\n        if (0 < oS.alphas[i]<oS.C):\n            oS.b = b1\n        elif (0 < oS.alphas[j]<oS.C):\n            oS.b = b2\n        else:\n            oS.b = (b1 + b2)/2.0\n        return 1\n    else:\n        return 0\n\n\n#SMO函数，用于快速求解出alpha\ndef smoP(dataMatIn, classLabels, C, toler, maxIter,kTup=('lin', 0)): #输入参数：数据特征，数据类别，参数C，阀值toler，最大迭代次数，核函数（默认线性核）\n    oS = optStruct(mat(dataMatIn),mat(classLabels).transpose(),C,toler, kTup)\n    iter = 0\n    entireSet = True\n    alphaPairsChanged = 0\n    while (iter < maxIter) and ((alphaPairsChanged > 0) or (entireSet)):\n        alphaPairsChanged = 0\n        if entireSet:\n            for i in range(oS.m): #遍历所有数据\n                alphaPairsChanged += innerL(i,oS)\n                print(\"fullSet, iter: %d i:%d, pairs changed %d\" % (iter,i,alphaPairsChanged)) #显示第多少次迭代，那行特征数据使alpha发生了改变，这次改变了多少次alpha\n            iter += 1\n        else:\n            nonBoundIs = nonzero((oS.alphas.A > 0) * (oS.alphas.A < C))[0]\n            for i in nonBoundIs: #遍历非边界的数据\n                alphaPairsChanged += innerL(i,oS)\n                print(\"non-bound, iter: %d i:%d, pairs changed %d\" % (iter,i,alphaPairsChanged))\n            iter += 1\n        if entireSet:\n            entireSet = False\n        elif (alphaPairsChanged == 0):\n            entireSet = True\n        print(\"iteration number: %d\" % iter)\n    return oS.b,oS.alphas\n\ndef testRbf(data_train,data_test):\n    dataArr,labelArr = loadDataSet(data_train) #读取训练数据\n    b,alphas = smoP(dataArr, labelArr, 200, 0.0001, 10000, ('rbf', 1.3)) #通过SMO算法得到b和alpha\n    datMat = mat(dataArr)\n    labelMat = mat(labelArr).transpose()\n    svInd = nonzero(alphas)[0]  #选取不为0数据的行数（也就是支持向量）\n    sVs = datMat[svInd] #支持向量的特征数据\n    labelSV = labelMat[svInd] #支持向量的类别（1或-1）\n    print(\"there are %d Support Vectors\" % shape(sVs)[0]) #打印出共有多少的支持向量\n    m,n = shape(datMat) #训练数据的行列数\n    errorCount = 0\n    for i in range(m):\n        kernelEval = kernelTrans(sVs,datMat[i,:],('rbf', 1.3)) #将支持向量转化为核函数\n        predict=kernelEval.T * multiply(labelSV,alphas[svInd]) + b  #这一行的预测结果（代码来源于《统计学习方法》p133里面最后用于预测的公式）注意最后确定的分离平面只有那些支持向量决定。\n        if sign(predict)!=sign(labelArr[i]): #sign函数 -1 if x < 0, 0 if x==0, 1 if x > 0\n            errorCount += 1\n    print(\"the training error rate is: %f\" % (float(errorCount)/m)) #打印出错误率\n    dataArr_test,labelArr_test = loadDataSet(data_test) #读取测试数据\n    errorCount_test = 0\n    datMat_test=mat(dataArr_test)\n    labelMat = mat(labelArr_test).transpose()\n    m,n = shape(datMat_test)\n    for i in range(m): #在测试数据上检验错误率\n        kernelEval = kernelTrans(sVs,datMat_test[i,:],('rbf', 1.3))\n        predict=kernelEval.T * multiply(labelSV,alphas[svInd]) + b\n        if sign(predict)!=sign(labelArr_test[i]):\n            errorCount_test += 1\n    print(\"the test error rate is: %f\" % (float(errorCount_test)/m))\n\n#主程序\ndef main():\n    filename_traindata='train_data.txt'\n    filename_testdata='test_data.txt'\n    testRbf(filename_traindata,filename_testdata)\n\nif __name__=='__main__':\n    main()\n```\n\n参考于:[参考地址](https://zhuanlan.zhihu.com/p/31886934)\n[参考地址2](https://zhuanlan.zhihu.com/p/46762820)\n# 贝叶斯分类\n自行了解贝叶斯公式和条件概率。\n## 贝叶斯代码\n```bash\nimport numpy as np\n \ndataMat=np.loadtxt(open('dataset.csv'), delimiter=',', skiprows = 1)#读取含有多个数组的文件\n# dataMat为读到的文件的所有数据\nrow = dataMat.shape[0]#数据行数\ncol = dataMat.shape[1]#数据列数 \n#print(row)\n#print(col)\n###注意下标从0开始的\n#print(dataMat[0,:])#打印第1行数据（即1行所有列）\n#print(dataMat[:,1])#打印第2列数据（即2列所有行）\n \n#假设女生现被有一男求婚，属性为：帅、性格不好、高、不上进，求嫁否？？？\n \n#根据数据集，下面开始计算一些属性（也可称为特征、症状）的先验概率\n \n#print(dataMat)\n#print(sum(dataMat[:,4]))\n \np_marry = sum(dataMat[:,4])/row #P(嫁)的概率\np_marry_not = 1-p_marry  #P(不嫁)的概率\n \n##函数，计算已知x的情况下，y事件发生的概率，即p(y|x)和p(y|~x)\n#传入：二维数组，x和y的值是列的索引值\n#返回四个值：嫁x情况下y事件发生与不发生的概率p(y|x)、p(~y|x)，和不嫁~x情况下y事件发生与不发生的概率p(y|~x)、p(~y|~x)。\ndef computP(data,x,y):\n    row = data.shape[0]#数据行数\n    col = data.shape[1]#数据列数 \n    marray = 0\n    marrayNot=0\n    yInMarray = 0\n    yInMarrayNot = 0 \n    for i in range(row):\n        if data[i,x] == 1: #嫁\n            marray = marray + 1\n            if data[i,y] == 1: #嫁情况下，y事件发生\n                yInMarray = yInMarray + 1\n        if data[i,x] == 0: #不嫁\n            marrayNot = marrayNot + 1\n            if data[i,y] == 1:#不嫁情况下，y事件发生\n                yInMarrayNot = yInMarrayNot+1\n    p_y_marray = yInMarray/marray #嫁情况下y事件发生的概率\n    p_n_marray = 1 - p_y_marray   #嫁情况下y事件不发生的概率\n    p_y_marrayNot = yInMarrayNot/marrayNot #不嫁情况下y事件发生的概率\n    p_n_marrayNot = 1- p_y_marrayNot  #不嫁情况下y事件不发生的概率\n    return p_y_marray,p_n_marray,p_y_marrayNot,p_n_marrayNot  \n#下面调用函数测试，4表示第5列、0表示第1列。返回四个概率值\npym,pnm,pymnot,pnmnot = computP(dataMat,4,0)#计算在第5列（嫁人与否）情况下，第1列的四个概率值。\n#print(ptm)\n#print(ptmn)\n \nif __name__ == \"__main__\":\n    print ('*****女生现被有一男求婚，男有属性为：帅、性格不好、高、不上进。')\n    print ('     根据已有数据经验，预测该女选择嫁否？')\n   # print(p_marry)\n    #假设女生现被有一男求婚，男有属性为：帅、性格不好、高、不上进，求该女选择嫁否？？？\n    # 下面分别计算上述属性，在嫁或不嫁时的四个条件概率值\n    p_shuai_marry,p_bushuai_marry,p_shuai_marryNot,p_bushuai_marryNot = computP(dataMat,4,0)\n    p_x_marry,p_xbad_marry,p_x_marryNot,p_xbad_marryNot = computP(dataMat,4,1)\n    p_g_marry,p_gnot_marry,p_g_marryNot,p_gnot_marryNot = computP(dataMat,4,2)\n    p_s_marry,p_snot_marry,p_s_marryNot,p_snot_marryNot = computP(dataMat,4,3)\n    \n    #嫁的情况下，上述属性发生的概率为p(嫁)*p(帅|嫁)*p(性格不好|嫁)*p(高|嫁)*p(不上进|嫁)\n    p_e_marry = p_marry*p_shuai_marry*p_xbad_marry* p_g_marry*p_snot_marry\n   #不嫁的情况下，上述属性发生的概率为p(不嫁)*p(帅|不嫁)*p(性格不好|不嫁)*p(高|不嫁)*p(不上进|不嫁)\n    p_n_marry = p_marry_not*p_shuai_marryNot*p_xbad_marryNot* p_g_marryNot*p_snot_marryNot\n    \n    #根据贝叶斯公式计算预测嫁或不嫁的概率\n    p_yuce_marry = p_e_marry/(p_e_marry+p_n_marry)\n    p_yuce_marryNot = p_n_marry/(p_e_marry+p_n_marry)\n    print(\"*****预测结果如下\")\n    print (\"预测嫁的概率是: %3.2f\" %p_yuce_marry)\n    print (\"预测不嫁的概率是: %3.2f\" %p_yuce_marryNot)\n\n```","tags":["答案"]},{"title":"移动测试","url":"/2020/06/30/移动测试/","content":"# 第一章基础知识\n## 1.四种测试用例\n功能性测试用例\n安全性测试用例\n性能压力测试用例\n兼容性测试用例\n## 2.测试方法\n等价划分⽅法：将所有输⼊划分为若⼲个⼦集，在每个⼦集中如果任意⼀个输⼊数据 对于揭露程序中潜在的错误都具有相同的效果，那么这样的⼦集就构成了⼀个等价 类。后续只要从每个等价类中任意选取⼀个值进⾏测试，就可以⽤少量具有代表性的 测试输⼊取得较好的测试覆盖效果； \n边界值分析⽅法： 是选取输⼊，输出的边界值进⾏测试，因为通常⼤量的软件错误是 发⽣在输⼊或者输出范围的边界上，所以需要对边界值进⾏重点测试，通常选取正好 等于，刚刚⼤于或公刚刚⼩于边界的值作为测试数据；\n错误推测⽅法：基于被测软件系统设计的理解，过往经验以及个⼈直觉，推测出软件 可能存在的缺陷，从⽽有针对性的设计测试⽤例的⽅法。强调对被测软件的需求理解 以及设计实现的细节把握，以及个⼈能⼒和经验。\n## selenium的八种定位方式\n❖ By.id() 通过id定位 \n❖ By.name() 通过name 定位 \n❖ By.xpath() 通过xpath定位 \n❖ By.className() 通过className定位 \n❖ By.cssSelector() 通过CSS 定位 \n❖ By.linkText() 通过linkText \n❖ By.tagName() 通过tagName \n❖ By.partialLinkText() 通过匹到的部分linkText\n# 第三章GUI自动化\n## selenium简介\nSelenium是⼀一个⽤用于Web应⽤用程序 ⾃自动化测试⼯工具。Selenium测试直 接运⾏行行在浏览器器中，就像真正的⽤用 户在操作⼀一样。Selenium并提供⼀一 套测试函数，⽤用于⽀支持Web⾃自动化 测试，函数⾮非常灵活，能够完成界 ⾯面元素定位、窗⼝口跳转、结果⽐比较 等。\n## selenium主要功能\n1）多浏览器⽀支持：可以对多浏览器进行测试，如支持的浏览器包括IE（7, 8, 9, 10, 11），Mozilla Firefox，Safari，Google Chrome，Opera等。\n2）支持多种语言：如Java、C#、Python、Ruby、PHP等。\n3）支持多种操作系统：如Windows、Linux、IOS、Android等。\n4）开源免费。\n## selenium工作原理步骤\n❖当使⽤用 Selenium 2.0 启动浏览器时，后台会同时启动基于 WebDriver Wire 协议的 Web Service 作为 Selenium 的 Remote Server，并与浏览器绑定。之后，Remote Server 就开始监听 Client 端的操作请求； \n❖执⾏行行测试时，测试⽤用例例会作为 Client 端，将需要执⾏行行的⻚页⾯面操作请求以 Http Request 的⽅方式发送给 Remote Server 。 该 Http Request 的 body，是以 WebDriver Wire 协议规定的 JSON 格式来描述需要浏览器执⾏行行的具体操作； \n❖Remote Server 接收到请求后，会对请求进⾏行行解析，并将解析结果发给 WebDriver，由WebDriver 实际执⾏行行浏览器的操作； \n❖WebDriver 可以看做是直接操作浏览器的原⽣生组件（Native Component），所以搭建测试环境时，通常都需要先下载浏览器对应的 WebDriver。\n## selenium使用http协议的原因\n• 因为HTTP协议是⼀个浏览器和Web服务器之间通信的标准协议，⽽⼏乎每⼀种编程语⾔ 都提供了丰富的http libraries，这样就可以⽅便的处理客户端Client和服务器Server之间的 请求request及响应response，WebDriver的结构中就是典型的C/S结构，WebDriver API 相当于是客户端，浏览器驱动是服务器端。\n## 浏览器驱动可以处理Java又可以处理Python脚本呢？\n因为WebDriver是基于JSON Wire Protocol的，它是在HTTP协议的基础上，对HTTP请求 及响应信息的BODY部分的数据进⼀步规范, BODY部分主要传送具体的数据，在 WebDriver中这些数据都是以JSON的形式存在并进⾏传送的，这就是JSON Wire protocol。所以在Client和Server之间，只要是基于JSON Wire Protocol来传递数据，就与 具体的脚本语⾔⽆关了，这样同⼀个浏览器的驱动就即可以处理Java语⾔的脚本，也可以 处理Python语⾔的脚本了。\n## 测试数据解耦和数据驱动\n如果测试脚本灵活性很低，且对于那些具有相同页面操作而只是输入的数据不同的用例来说，存在大量重复代码。\n解决：将测试脚本与测试数据分为两个文件，将测试数据存为表格，每读一行就将这一行数据传给脚本并得到测试结果。于是乎表格文件多少行测试用例就会执行几次。\n这种数据驱动的思想还适用于API测试，接口测试，单元测试等。\n## 数据驱动的模块化思想\n❖ 测试数据和测试脚本分离；\n❖ 模块化思想：通⽤操作打包成函数（登录，登出，搜 索）\n❖ 以页⾯为对象模型：class XXpage{}\n## pytest的特点\n❖ 简单灵活，容易上⼿； \n❖ ⽀持参数化； \n❖ 能够支持简单的单元测试和复杂的功能测试，还可以用来做selenium/appnium等自动化测试、接口自动化测试（pytest+requests）; \n❖ pytest具有很多第三方插件，并且可以⾃定义扩展，比较好用的如pytestselenium（集成selenium）、pytest-html（完美html测试报告⽣成）、pytestrerunfailures（失败case重复执⾏）、pytest-xdist（多CPU分发）等； \n❖ 测试⽤例的skip和xfail处理； \n❖ 可以很好的和jenkins集成。\n## pytest使用准则\n• pytest只能识别以test_开头和_test结尾的.py⽂件，识别以test.开头的所有方法、类 ；\n• 测试类必须以Test开头，并且不能有  __init__方法；\n• 如果当前文件夹下有多个文件，test_sample2.py ， test_sample3.py 想 要运行所有的测试用例，只需要运行命令：pytest就可以了。如果运行单个文件，则运行命令： pytest test_sample2.py ；\n• 断言使用基本的assert即可。\n## 断言的意义\n断言是写自动化测试基本最重要的一步，一个用例没有断言，就失去了自动化测试的意义。\n简单来讲就是实际结果和期望结果去对比，符合预期那就测试pass，不符合预期那就测试 failed。\n# 第五章http协议\n## web服务器\n是指能够为发出请求的浏览器提供文档的程序。服务器是一种被动的程序，只有当我们从浏览器发出请求时才会响应。 （写⽹网站：是需要把我们所写的代码部署到web服务器上，这样就可以让全世界的 ⼈人都能够去访问你的网站，你那⾥里里⾯面放置的所有的数据、资源，大家都可以下载。目前主流的三个Web服务器是Apache，Nginx，IIS）\n## web服务器和URL\nHTTP使用统一资源标识符（Uniform Resource Identiﬁers, URI）来传输数据和建立连接。\nURL是一种特殊类型的URI，包含了用于查找某个资源的足够的信息。\nURL,全称是UniformResourceLocator, 中文叫统一资源定位符,是互联网上用来标识某一处资源的地址。\n## URL分段分析\n对于这个url：\n**http://www.aspxfans.com:8080/news/index.asp?boardID=5&ID=24618&page=1#name**\n❖ 协议部分：该URL的协议部分为“http：”，这代表网页使用的是HTTP协议。在Internet中可以使用多种协议，如HTTP，FTP等本例中使用的是HTTP协议。在\"HTTP\"后面的“//”为分隔符。\n❖ 域名部分：该URL的域名部分为“www.aspxfans.com”。一个URL中，也可以使⽤用IP地址作为域名使用。\n❖  端口部分：跟在域名后⾯面的是端口，域名和端口之间使用“:”作为分隔符。端口不是一个URL必须的部分， 如果省略略端口部分，将采用默认端口。\n❖  虚拟目录部分：从域名后的第一个“/”开始到最后一个“/”为止，是虚拟目录部分。虚拟目录\n也不不是一个 URL必须的部分。本例中的虚拟目录是“/news/”。\n❖  文件名部分：从域名后的最后一个“/”开始到“？”为止，是文件名部分，如果没有“?”,则是从域名后的最后一个“/”开始到“#”为⽌止，是文件部分，如果没有“？”和“#”，那么从域名后的最后一个“/”开始到结束，都 是文件名部分。本例例中的文件名是“index.asp”。文件名部分也不不是一个URL必须的部分，如果省略该部分，则使用默认的文件名。\n❖  锚部分：从“#”开始到最后，都是锚部分。本例例中的锚部分是“name”。锚部分也不不是一个URL必须的部分\n❖  参数部分：从“？”开始到“#”为止之间的部分为参数部分，又称搜索部分、查询部分。本例例中的参数部分为：\n“boardID=5&ID=24618&page=1”\n## http协议\nHTTP协议（HyperText Transfer Protocol，超文本传输协议）是因特网上应用最为广泛的一种网络传输协议，所有的WWW文件都必须遵守这个标准。\nHTTP是一个基于TCP/IP通信协议来传递数据（HTML文件，图片文件，查询结果等）。\n## 协议原理\nHTTP协议定义Web客户端如何从Web服务器请求Web页面，以及服务器如何把Web⻚页⾯面传送给客户端。HTTP协议采用了了请求/响应模型。客户端向服务器发送一个请求报文，请求报文包含请求的方法、URL、协议版本、请求头部和请求数据。服务器以一个状态行作为响应，响应的内容包括协议的版本、成功或者错误代码、服务器信息、响应头部和响应数据。\n## 浏览器地址栏键入URL回车后发生的流程\n• 浏览器向DNS服务器请求解析该URL中的域名所对应的IP地址; \n• 解析出IP地址后，根据该IP地址和默认端口 80，和服务器建立TCP连接; \n• 浏览器发出读取文件(URL 中域名后面部分对应的文件)的HTTP请求，该请求报文作为TCP三次握手的第三个报文的数据发送给服务器; \n• 服务器对浏览器请求作出响应，并把对应的 html文本发送给浏览器; \n• 释放 TCP连接; \n• 浏览器将该html文本并显示内容;\n## http发展历史\n{% img /img/httpfazhan.png 400 400 %}\n## 七层OSI模型和TCP/IP模型\n{% img /img/httpcen.png 400 400 %}\n## http的特点\n• 基于请求—响应的模式：HTTP协议规定,请求从客户端发出,最后服务器端响应 该请求并返回。换句句话说,肯定是先从客户端开始建立通信的,服务器端在没有接收到请求之前不会发送响应。 \n•无状态保存：HTTP是一种不保存状态,即无状态(stateless)协议。HTTP协议自身不对请求和响应之间的通信状态进行保存。也就是说在HTTP这个级别,协议对于发送过的请求或响应都不做持久化处理。HTTP/1.1虽然是无状态协议,但为了了实现期望的保持状态功能, 于是引入了Cookie技术。有了了Cookie再用HTTP协议通信,就可以管理状态了。\n•无连接：无连接的含义是限制每次连接只处理理一个请求。服务器处理完客户的请求，并收到客户的应答后，即断开连接。采用这种方式可以节省传输时间，并且可以提高并发性能，不能和每个用户建立长久的连接，请求一次相应一次，服务端和客户端就中断了。\n注意：早期的http协议是一个请求一个响应之后，直接就断开了，但是现在的http协议1.1版本不是直接就断开，而是等几秒钟，这几秒钟是等着用户如果有后续的操作与否，如果用户在这几秒钟之内有新的请求，那么还是通过之前的连接通道来收发消息，如果过了这几秒钟用户没有发送新的请求，那么就会断开连接，这样可以提高效率，减少短时间内建立连接的次数。\n## http的请求\n• HTTP请求协议包括：请求行（request line）、请求头部（header）、空行和请求数据四个部分组成。 \n• 请求行以一个方法符号开头，以空格分开，后面跟着请求的URI和协议的版本。\n## http的请求方式\nHTTP1.1定义请求方法：GET, POST, HEAD,OPTIONS, PUT, DELETE, TRACE 和 CONNECT。\n{% img /img/fangshi.png 400 400 %}\n## get和post的区别\n1）GET提交的数据会放在URL之后，以?分割URL和传输数据，参数之间以&相连，例如 ：EditPosts.aspx?name=test1&id=123456。POST方法是把提交的数据放在HTTP包的 Body中。\n2）GET提交的数据大小有限制（因为浏览器对URL的长度有限制），而POST方法提交的数据没有限制。\n3）GET方式提交数据，会带来安全问题，比如一个登录页面，通过GET方式提交数据时，用 户名和密码将出现在URL上，如果页面可以被缓存或者其他人可以访问这台机器，就可以从历史记录获得该用户的账号和密码。\n## http状态码\n状态代码有三位数字组成，第一个数字定义了了响应的类别，共分五种类别: \n1xx：指示信息--表示请求已接收，继续处理。\n2xx：成功--表示请求已被成功接收、理解、        接受。\n3xx：重定向--要完成请求必须进行更进一步的操作。 \n4xx：客户端错误--请求有语法错误或请求无法实现。\n5xx：服务器端错误--服务器未能实现合法的请求。\n常见状态码：\n200 OK   //客户端请求成功。 \n400 Bad Request  //客户端请求有语法错误，不能被服务器所理理解。 \n401 Unauthorized   //请求未经授权，这个状态代码必须和WWW-Authenticate报头域一起使用。 \n403 Forbidden   //服务器收到请求，但是拒绝提供服务。 \n404 Not Found   //请求资源不存在。eg：输入了错误的URL。\n500 Internal Server Error   //服务器发⽣生不可预期的错误。\n503 Server Unavailable   //服务器当前不能处理客户端的请求，一段时间后可能恢复正常。\n## http的不足\n❖ 通信使用明文，内容可能会被窃听 —— 加密通信线路。 \n❖ 不验证通信方，可能遭遇伪装 —— 证书。 \n❖ ⽆法验证报文的完整性，可能已被篡改 —— 数字签名。\n## https介绍\nHTTP协议中没有加密机制,但可以通过和SSL(Secure Socket Layer,安全套接层 )或TLS(Transport Layer Security,安全层传输协议)的组合使用,加密HTTP的通信内容。属于通信加密，即在整个通信线路中加密。\nHTTP + 加密 + 认证 + 完整性保护 = HTTPS (HTTP Secure)\n{% img /img/anquan.png 400 400 %}\n## https加密认证\nHTTPS 采⽤共享密钥加密（对称）和公开密钥加密（⾮对称）两者并⽤的混合加密制。若密钥能够实现安全交换,那么有可能会考虑仅使⽤公开密钥加密来通信。但是公开密钥加密与共享密钥加密相⽐,其处理速度要慢。\n## 对称加密和非对称加密\n对称加密的特点是文件加密和解密都是使用相同的密钥，因此密钥需要在加密方和解密方传输，导致密钥容易泄露和被破解。步骤如下：\n1）甲方选择某一种加密规则，对信息进行加密；\n2）乙方使用同一种规则，对信息进行解密。\n非对称加密算法需要两个密钥：公开密钥（publickey）和私有密钥（privatekey）。公开密钥与私有密钥是一对，如果用公开密钥对数据进行加密，只有用对应的私有密钥才能解密， 根据公开的公钥无法测算出另一不公开的密钥（即为私钥），而私有密钥保存在解密方本地，只要解密方自己不泄露出去，私钥就没有泄露的可能。非对称加密很好地克服了对称加密算法的弱点。\n## https的不足\n❖ 加密解密过程复杂，导致访问速度慢； \n❖ 加密需要认向证机构付费； \n❖ 整个页⾯的请求都要使⽤HTTPS。\n# 第六章fiddler抓包\n1) 客户端像WEB服务器发送HTTP(S)请求时，请求会先经过Fiddler代理服务器。\n2) Fiddler代理服务器截取客户端的请求报文，再转发到WEB服务器，转发之前可以做一些请求报文参数修改的操作。\n3) WEB服务器处理完请求以后返回响应报文，Fiddler代理服务器会截取WEB服务器的响应报文。\n4) Fiddler处理完响应报文后再返回给客户端。\n# 第七章cookie和session\n## 什么是cookie和session\n在程序中，会话跟踪是很重要的事情。理论上，⼀个⽤户的所有请求操作都应该属于 同⼀个会话，⽽另⼀个⽤户的所有请求操作则应该属于另⼀个会话，⼆者不能混淆。 例如，⽤户A在超市购买的任何商品都应该放在A的购物车内，不论是⽤户A什么时间 购买的，这都是属于同⼀个会话的，不能放⼊⽤户B或⽤户C的购物车内，这不属于同 ⼀个会话。⽽Web应⽤程序是使⽤HTTP协议传输数据的。HTTP协议是⽆状态的协 议。⼀旦数据交换完毕，客户端与服务器端的连接就会关闭，再次交换数据需要建⽴ 新的连接。这就意味着服务器⽆法从连接上跟踪会话。即⽤户A购买了⼀件商品放⼊购 物车内，当再次购买商品时服务器已经⽆法判断该购买⾏为是属于⽤户A的会话还是⽤ 户B的会话了。要跟踪该会话，必须引⼊⼀种机制。Cookie就是这样的⼀种机制。它可 以弥补HTTP协议⽆状态的不⾜。在Session出现之前，基本上所有的⽹站都采⽤ Cookie来跟踪会话。\n## session和cookie的作用是？\n会话（Session）跟踪是Web程序中常用的技术，用来跟踪用户的整个会话。常用的会话跟踪技术是Cookie与Session。Cookie通过在客户端记录信息确定用户身份。\n## cookie是什么？\nCookie意为“甜饼”，是由W3C组织提出，最早由Netscape社区发展的⼀种机制。⽬前 Cookie已经成为标准，所有的主流浏览器如IE、Netscape、Firefox、Opera等都⽀持 Cookie。 　　Cookie实际上是⼀⼩段的⽂本信息。客户端请求服务器，如果服务器需要记录该⽤ 户状态，就使⽤response向客户端浏览器颁发⼀个Cookie。客户端浏览器会把Cookie保存起来。当浏览器再请求该⽹站时，浏览器把请求的⽹址连同该Cookie⼀同提交给服务器。服务器检查该Cookie，以此来辨认⽤户状态。服务器还可以根据需要修改Cookie的内容。\n## 什么是session？\nSession是服务器端使⽤的⼀种记录客户端状态的机制，使⽤上⽐Cookie简单⼀些，相应的也增加了服务器的存储压⼒。\nSession是另⼀种记录客户状态的机制，不同的是Cookie保存在客户端浏览器中，而Session保存在服务器上。客户端浏览器访问服务器的时候，服务器把客户端信息以某种形式记录在服务器上。这就是Session。客户端浏览器再次访问时只需要从该Session 中查找该客户的状态就可以了。如果说Cookie机制是通过检查客户身上的“通行证”来 确定客户身份的话，那么Session机制就是通过检查服务器上的“客户明细表”来确认客户身份。Session相当于程序在服务器上建立的一份客户档案，客户来访的时候只需要查询客户档案表就可以了。\n## cookie和session的区别\n❖ cookie数据存放在客户的浏览器上，session数据放在服务器上. \n❖ cookie不是很安全，别⼈可以分析存放在本地的COOKIE并进⾏COOKIE欺骗。考虑到 安全应当使⽤session。\n❖ session会在⼀定时间内保存在服务器上。当访问增多，会⽐较占⽤你服务器的性能考 虑到减轻服务器性能⽅⾯，应当使⽤cookie。 \n❖ 单个cookie保存的数据不能超过4K，很多浏览器都限制⼀个站点最多保存20个cookie。 (Session对象没有对存储的数据量的限制，其中可以保存更为复杂的数据类型)。\n❖ 注：记住密码功能就是使⽤永久cookie写在客户端电脑硬盘上，下次登录时，⾃动将 cookie信息附加发送给服务端。\n# 第九章git\n## git基本操作\ngit status 以查看在你上次提交之后是否有修改。\ngit add 命令可将该文件添加到缓存。\ngit init 在目录中创建新的 Git 仓库。 你可以在任何时候、任何目录中这么做，完全是本地化的。\n使用 git clone 拷贝一个 Git 仓库到本地，让自己能够查看该项目，或者进行修改。\ngit commit 提交\n# 第十章Jenkins\n## Jenkins是什么\nJenkins是⼀个开源的、提供友好操作界⾯的持续集成(CI)⼯具，主要⽤于持续、⾃动的构建/测试 软件项⽬、监控外部任务的运行。\n## CI/CD是什么\nCI(Continuous integration，中文意思是持续集成)是⼀种软件开发时间。持续集成强调开发⼈员提交了新代码之后，立刻进⾏构建、（单元）测试。根据测试结果，我们可以确定新代码和原有代码能否正确地 集成在一起。\nCD(Continuous Delivery， 中⽂意思持续交付)是在持续集成的基础上，将 集成后的代码部署到更贴近真实运⾏环境(类⽣产环境)中。⽐如，我们完成 单元测试后，可以把代码部署到连接数据库的Staging环境中更多的测试。 如果代码没有问题，可以继续⼿动部署到⽣产环境。下图反应的是CI/CD 的⼤概⼯作模式。\n# 第十一章性能测试\n## 1.什么是软件性能？\n定义：软件的性能是软件的一种非功能特性，它关注的不是软件是否能 够完成特定的功能，⽽是在完成该功能时展⽰出来的及时性。\n由定义可知性能关注的是软件的⾮功能特性，所以⼀般来说性能测试介 ⼊的时机是在功能测试完成之后。另外，由定义中的及时性可知性能也 是⼀种指标，可以⽤时间或其它指标来衡量，通常我们会使⽤某些⼯具 或⼿段来检测软件的某些指标是否达到了要求，这就是性能测试。\n## 2.什么是性能测试：\n性能测试定义：指通过⾃动化的测试⼯具模拟多种正常、峰值以及异常 负载条件来对系统的各项性能指标进⾏测试 。\n测试⼈员在做性能测试时除开要关注表⾯的现象如响应时间，也需要关注本质，比如用户看不到的服务器资料利用率，架构设计是否 合理？代码是否合理等方面。\n## 3.性能测试主要测试什么：\n• 基准测试：在给系统施加较低压力时，查看系统的运行状况并记录 相关数做为基础参考。 \n• 负载测试：是指对系统不断地增加压力或增加⼀定压力下的持续时间，直到系统的某项或多项性能指标达到安全临界值，例如某种资源已经达到饱和状态等 。 \n• 压力测试：压力测试是评估系统处于或超过预期负载时系统的运行情况，关注点在于系统在峰值负载或超出最大载荷情况下的处理能力。 \n• 稳定性测试：在给系统加载⼀定业务压力的情况下，使系统运行一段时间，以此检测系统是否稳定。 \n• 并发测试：测试多个用户同时访问同一个应用、同一个模块或者数 据记录时是否存在死锁或者其他性能问题。\n## 4.性能测试指标及其意义（概念）：\n❖ 响应时间：是指系统对请求作出响应的时间，可以理解为是指⽤户从客户端发起一个请求开始，到客户端接收到从服务器端返回的响应结束，整个过程所耗费的时间。直观上看，这个指标与⼈对软件性能的主观感受是⾮常⼀致的，因为它完 整地记录了整个计算机系统处理请求的时间。\n❖ 系统处理能力：系统处理能⼒是指系统在利⽤系统硬件平台和软件平台进⾏信息处 理的能⼒。系统处理能⼒通过系统每秒钟能够处理的交易数量来评价。\n⼀般情况下，系统处理能⼒又⽤以下⼏个指标来度量： \nHPS（Hits Per Second）：每秒点击次数，单位是次/秒； \nTPS（Transaction per Second）：系统每秒处理交易数，单位是笔/秒； \nQPS（Query per Second）：系统每秒处理查询次数，单位是次/秒。\n❖ 吞吐量：吞吐量是指系统在单位时间内处理请求的数量。对于单用户的系统，响应时间可以很好地度量系统的性能，但对于并发系统，通常需要用吞吐量作为性能指标。\n❖ 并发数：并发用户数指在同一时刻内，登录系统并进⾏业 务操作的用户数量。与吞吐量相⽐，并发用户数是一个更 直观但也更笼统的性能指标。实际上，并发用户数是一个⾮常不准确的指标，因为用户不同的使⽤模式会导致不同用户在单位时间发出不同数量的请求。\n❖ 错误率：一批请求中结果出错的概率。错误率＝(失败交易 数/交易总数)*100%。不同系统对错误率的要求不同，但一般不超出千分之六，即成功率不低于99.4%\n","tags":["答案"]},{"title":"hive_2（曦哥版）","url":"/2020/06/28/hive-2/","content":"## 选择题\n1\thive元数据储存的三种模式不包括那个（C）\t\n\tA、\t单用户模式\n\tB、\t多用户模式\n\tC、\t近程服务器模式\n\tD、\t远程服务器模式\n\t\t\n2   Hive支持的分割符，常用的有几种(C)\t\n\tA、\t1\n\tB、\t3\n\tC、\t5\n\tD、\t4\n\t\t\n3\t常用分隔符不包括以下哪个（D）\t\n\tA、\t\\n\n\tB、\t^A\n\tC、\t^B\n\tD、\t^D\n\t\t\n4\tHive提供的哪种客户端访问接口，客户端可以直接在命令行模式下进行操作（A）\t\n\tA、\tHive CLI\n\tB、\thwi\n\tC、\thiveserver\n\t\t\n5\thive中主要包含几种数据模型（C）\t\n\tA、\t1\n\tB、\t2\n\tC、\t3\n\tD、\t4\n\t\t\n6\thive复杂的数据类型不包括以下哪个（A）\t\n\tA、\tstring\n\tB、\tarray\n\tC、\tmap\n\tD、\tstruct\n\n7  以下哪个原子数据类型为1字节（A）\t\n\tA、\tTINYINT\n\tB、\tSAMLLINT\n\tC、\tINT\n\tD、\tBIGINT\n8\t在将数据导出的时候，若数据是用户所需要的文件格式，下列哪个语法是正确的？（D）\n    A.\tHadoop -cp  fs  源文件路径 目标路径\n    B.\tHadoop -cp  fs  目标路径 源文件路径\n    C.\tHadoop fs  -cp  目标路径 源文件路径\n    D.\tHadoop fs  -cp  源文件路径 目标路径\n\n9\t若想要将数据加载到Hive外部表中，有几种方式？（B）\n    A. 1种\n    B. 2种\n    C. 3种\n    D. 4种\n\n10\t下列哪些是能修改的数据库信息？（D）\n    A 元数据 \n    B 数据库名 \n    C 数据库所在目录位置 \n    D 键值对属性值\n\n11\t使用Load DATA 命令，数据会被转移或复制到_____（B）\n    A 本地文件系统路径 \n    B 分布式文件系统路径 \n    C 磁盘 \n    D 内存\n\n12\t通过查询语句向表中插入数据应该使用什么语句？（C）\n    A. SELECT...WHERE 语句 \n    B.ALTER TABLE 语句 \n    C.INSERT语句 \n    D.INSERT.....DIRECTORY语句\n\n13\t在装载数据到表中的过程中，Hive会验证以下什么？（B）\n    A.验证用户装载的数据和表的模式是否匹配   \n    B.验证文件格式是否和表结构定义的一致          \n    C. 两者都要验证         \n    D.两者都不验证\n\n14\tOVERWRITE关键字和INTO关键字的意义是？（B）\n    A.把新增的文件增加到目标文件夹 \n    B.以追加的方式写入数据而不会覆盖掉之前已经存在的内容 \n    C.两者都有 \n    D.以上都不正确\n\n15  对于INPATH字句中使用的文件路径有一个限制，是什么？（A）\n    A. 此路径下不可以包含任何文件夹  \n    B.此路径只能包含一个文件夹                    \n    C. 此路径下最多包含5个文件夹    \n    D.以上均不正确\n\n16  hive的基本数据类型不包括（D）\n    A，int  B,Bigint  C,String  D,Map\n\n17  hive的集合数据类型不包括（B）\n    A，truct  B,Boolean  C,Map  D,Array  \n18  下面（A）是二进制类型：\nA，binary  B,Boolean  C,int  D,Array  \n19  hive默认使用的行分隔符是（A）\nA，\\n  B, ^A  C, ^B  D, \\t\n20 用于分隔ARRARY或者STRUCT中的元素，或用于MAP中键-值对之间的分隔。在CREATE TABLE语句中可以使用八进制编码\\002表示：(C)\nA，\\n  B, ^A  C, ^B  D, \\t\n21 Hive建表方式不包括: (C)\nA，直接建表法  B,查询建表法  C,as建表法  D,like建表法\n22 create table partiton_table(sid int,sname string) partitioned by (gender string) row format delimited fields terminated by ',';这段是创建了一个（C）\nA，内部表  B,外部表  C,分区表  D,关系表\n23 Hive 中所有的数据都存储在 HDFS 中，以下数据模型中不属于hive的是：（D）\nA，内部表  B,外部表  C,分区表  D, 关系表\n24 hive建表时不指定格式时，默认格式为：（A）\nA，TextFile  B, SequenceFile  C,MapFile  D, ORCFile\n25 下列哪一个不是创建内部表：（C）\nA：create table t1(tid int,tname string, gender string); \nB: create table t2(tid int,tname string, gender string) row format delimited fields terminated by ','; \nC：create table t3(tid int,tname string) partitioned by (gender string) row format delimited fields terminated by ',';\nD：create table t4(tid int,tname string, gender string) as select * from t1;\n26 运算符~A数值类型，该如何描述？D\nA.A除以B。如果能整数除则返回商数 \nB.A和B按位取余\nC.A和B按位取与\nD.A按位取反\n27 查询语句中包含有（1-deductons[...]）这个运算，因为deductions字段是FLOAT类型的,因此数字1会提升为什么类型。C\nA.DOUBLE\nB.BIGINT\nC.FLOAT\nD.INT\n28 假设有一个表表示用户偏好，另一个表表示新闻文章，同时有一个算法会推测出用户可能会喜欢读哪些文章。这个时候就需要使用什么来生成所有用户和所有网页的对应关系的集合。B\nA.map-side JOIN\nB.笛卡尔积 JOIN\nC.RIGHT OUTER JOIN\nD.FULL OUTER JOIN\n29 哪个连接会返回左边表的记录，前提是其记录对于右边表满足ON语句中的判定条件。C\nA.笛卡尔积 JOIN\nB.RIGHT OUTER JOIN\nC.LEFT SEMI JOIN\nD.FULL OUTER JOIN\n30 Hive可以在map端执行连接过程，如果所有表中只有一个表是小表，那么可以在最大的表通过什么的时候将小表完全放到内存中。A\nA.mapper\nB.reduce\nC.select\nD.join\n31 DISTRIBUTE BY和GROUP BY在控制reducer如何接受一行行数据进行处理方面是类似的，而SORT BY则控制着什么的数据如何进行排序。A\nA.reduce内\nB.sort\nC.cluster\nD.以上都不是\n32 因为ORDER BY操作可能会导致运行时间过长，如果属性hive.mapred.mode的值是什么的话，那么Hive要求这样的语句必须加有LIMIT语句进行限制。B\nA.nonstrict\nB.strict\n33 SELECT和WHERE语句中不能引用什么表中的字段？B\nA.左边表\nB.右边表\n34 笛卡尔积是一种连接，表示左边表的行数怎么右边表的行数等于笛卡尔结果集的大小。D\nA.除以\nB.相加\nC.相减\nD.乘以\n35 左外连接通过关键字LEFT OUTER进行标识，在这种JOIN连接中，JOIN操作符左边表中符合WHERE子句的所有记录将会被返回。JOIN操作符右边表如果没有符合ON后面连接条件的记录时，那么从右边表制定选择的列的值将会是什么？C\nA.ALL\nB.NON\nC.NULL\nD.都不对\n\n### 多选\n1   Hive常见的数据导入方式有以下哪些（ABCD）\t\n\tA、\t从本地文件系统导入数据到hive表\n\tB、\t从hdfs上导入数据到hive表\n\tC、\t从其它表中查询出相应的数据并导入到hive表中\n\tD、\t在创建表的时候通过从其它的表中查询出相应的记录并插入到所创建的表中\n\t\t\n2   hive数据分为以下包括的是（BC）\t\n\tA、\t单数据\n\tB、\t元数据\n\tC、\t表数据\n\tD、\t多数据\n\n3   Hive数据库可支持如下哪些操作？（ABC）\n    A.创建 B.删除 C.选择 D.复制\n\n4 下列哪个是正确的？（ABCD）\n    A. Hive.exec.dynamic.partition的默认值是false\n    B. Hive.exec.dynamic.partition.mode的默认值是strict                    \n    C. Hive.exec.max.dynamic.partition.penode的默认值是1000\n    D. Hive.exec.max.created.files的默认值是10000   \n## 判断题\n1、元数据是用来存储表的名字、列、分区及其属性以及表的数据所在目录等（√）\n2、hive的数据存储在Hadoop分布式文件系统中（√）\n3、hive元数据储存模式中的多用户模式不需要本地运行一个MySQL服务器（X）\n4、hive原子数据类型中int类型为2字节（X）\n5、hive原子数据类型中double类型为8字节（√）\n6、hive的string数据类型相当于数据库的varchar数据类型（X）\n7、hive和关系数据库存储文件的系统相同（X）\n8、hive很容易扩展自己的储存能力和计算能力（√）\n9、hive元数据中的存储模式其中之一的单用户模式是hive默认的存储模式（√）\n10、表数据是hive中表格具有的数据（√）\n\n1、Hive数据库中的数据库名可以修改。（X）\n2、HIve数据库中的数据库属性信息可以修改。（√）\n3、删除外部表只删除元数据信息。（√）\n4、有external的情况下，不管原表是管理表还是外部表，新生成的表都是外部表。（X）\n5、分区表中分区结构就是HDFS的分区目录。（√）\n6、删除分区表后，元数据和数据将一并被删除，但HDFS对应的分区目录会被保留以便于查询。（X）\n7、设置hive.exec.dynamic.partition.mode为true,可实现使用动态分区功能。（X）\n8、Hive中不允许主分区为动态分区。（√）\n9、Hive中可设置部分列为动态分区，也可以设置所有列为动态分区。（√）\n10、分桶表参数之hive.mapred.mode的默认值是strict模式（√）\n\n1、函数floor、round和ceil（向上取整）输入的是DOUBLE类型的值，而返回的值是BIGINT类型的，也就是将浮点型数转换成整型了。（对）\n2、返回值类型是DOUBLE，样式为degrees（DOUBLE d）将DOUBLE型角度值d转换成弧度值，结果是DOUBLE型的。（错，应该是弧度值转换成角度值）\n3、算数运算符接受任意的数值类型。（对）\n4、如果参与运算的两个数据的数据类型不同，那么两种类型中值的范围较小的那个数据类型将转换为值更大的数据类型。（对）\n5、用户不但可以选择则表中的列，还可以使用函数调用和算术表达式来操作列值。（对）\n6、对于INT类型和BIGINT类型计算，INT类型会转换提升为BIGINT类型；对于INT类型和FLOAT类型运算，INT类型将提升为FLOAT类型的。（对）\n7、CASE...WHEN...THEN语句和if条件语句类似，用于处理多个列的查询结果。（错，处理单个列）\n8、HAVING子句允许用户通过一个简单的语法完成原本需要通过子查询才能对FROUP BY语句产生的分组进行条件过滤的任务。（对）\n9、内连接中，不是只有进行连接的两个表中都存在与链接标准相匹配的数据才会被保留下来。（错，只有两个表都存在才可以）\n10、Hive可以在map端执行连接过程（称为map-side JOIN），这是因为Hive可以和内存中的小表进行逐一匹配，从而省略掉常规连接操作所需要的reduce过程。（对）\n\n1、hive的内置数据类型分为基本数据类型和集合数据类型。（√）\n2、hive中数据模型主要有5种（×）\n3、对于hive的string类型相当于数据库的varchar类型，该类型是一个可变的字符串，不过它不能声明其中最多能存储多少个字符。（√）\n4、hive中默认使用^A(ctrl+A)作为列分割符，如果用户需要指定的话，等同于row format delimited fields terminated by '\\001'（√）\n5、建表的时候一定要根据结构化数据文件的分隔符类型指定分隔符（√）\n6、create table创建一个指定名字的表，如果相同名字的表已经存在，则会覆盖原先的表并清空里面的内容（×）\n7、hive创建外部表时，会将数据移动到数据仓库指向的路径；（×）\n8、桶表是对某一字段进行哈希取值后，放到不同的文件中，从而可以提高查询效率(√)\n9、分区表通常分为静态分区表和动态分区表，前者需要导入数据时静态指定分区，后者可以直接根据导入数据进行分区。（√）\n10、分桶表可以在分区表的基础之上创建，但不可以只创建分桶表。（×）\n## 解答题\n1、Hive数据仓库系统中，支持哪些数据类型？\n基本数据类型和集合数据类型\n2、Hive数据仓库中提供几种分割符，分别是什么？\n5种常用分割符\n分隔符：\\n，^A，^B，^C，\\t\n3、Hive中包含几种数据模型，分别是哪些？\n主要包含表、外部表、分区表和桶四种模型\n4、Hive中提供的几种文件格式，分别是哪些\nTextFile\nSequenceFile\nRCFile\nORC\n5、简述读时模式和写时模式的优缺点\n读时模式:\n优点：写数据很快，load data 效率高\n缺点：读数据比较慢，需要检查数据，解析字段和schema。\n写时模式:\n优点：读非常快\n缺点：写速度很慢\n6、hive提供了哪三种客户端用户访问接口？\nhive CLI\nHWI\nHiveserver\n7、Hive中表的数据默认存储于哪？\nHDFS\n8、hive元数据的三种储存模式\n单用户模式\n多用户模式\n远程服务器模式\n9、简述Hadoop与Hive间关系。\n“hive是hadoop的延申。 hadoop是一个分布式的软件处理框架,hive是一个提供了查询功能的数据仓库。\n1、 hive的原子数据类型可以转换吗？如果可以用什么方法转换呢？\n答：可以。隐式类型转换或者CAST操作显示进行数据类型转换\n2、hive建表的字段顺序和字段类型要跟结构化数据文件的数据类型一致吗？为什么？\n答：要一致。如果类型不一致，hive会尝试转换，不保证转换成功，如果成功就显示，如果失败就显示为null\n5、map keys terminated by ':'请简述这段代码的意思：\n答：集合类型数据元素之间使用英文冒号分隔\n6、请创建一个student_ext的外部表含有id，name,sex的信息，以'，'结尾的行格式分隔字段，且数据在hdfs根目录下的/test/stu\n答：\nCreate external table student_ext(id string,name string,birthday string,sex string) row format delimited fields terminated by ’，'location ’/test/stu’;\n7、请问通过create table test1 like test创建的test1和通过\nCreate table test2 AS SELECT id,name,gfs,adress from test创建的test2的表是否一样，如果不是，请问差别在哪里？\n答：不一样。test1只创建表结构，test2不仅创建相应的表结构，并且会插入数据。\n8、hive种内部表和和外部表的创建和删除有哪些区别：\n答：hive创建内部表时，会将数据移动到数据仓库指向的路径，删除表的时候元数据和数据会被一起删除。\nhive创建外部表时，烬记录数据所在的路径，不对数据的位置做任何改变，，删除表的时候只删除元数据，不删除数据。\n1：Hive中内部表如何标识，内部表有哪些操作？\n   内部表创建和传统数据库一样，没有标识关键字；内部表有create、drop、show等操作。\n2：怎么操作可以显示当前所在的数据库？\n   Set hive.cli.print.current.db=true\n3:  create...as....select语法为什么不能应用于外部表？\n  因为外部表本身没有进行数据装载，只有表结构和元数据。\n4：为什么要进行分桶？\n取样更高效，以及获得更好的查询处理效率。\n5：请写出桶抽样的的语法，并说明参数X和Y分别代表什么含义？\n语法：tablesample（bucket x out of y on）y决定抽样比例，即：抽取桶样本个数；x决定起始样本，即：从哪个桶开始选取。\n6：请写出“创建表student，按照line分区，按照uid分为32分桶”语句\nCreate table student（\nuid INT,\nLine STRING)\nPARTITIONED BY(line STRING)\nCLUSTERED BY(uid) BY(uid ASC) INTO 32 BUCKETS;\n7:简述Hive中内部表的内容\n内部表类比于数据库中表， 每个内部表在Hive中都有一个对应的存储数据的目录，可以删除表，删除元数据和数据\n1·\t用代码说明：①如何删除数据库，②以及当数据库中存在表时能否删除 以及用文字说明在什么情况下不能删除\n①drop database if exists myhive ②drop database if exits myhive cascade 默认情况下为restrict，不能删除。\n2·\t写出代码以实现创建数据库的操作\nCREATE (DATABASE|SCHEMA) [IF NOT EXISTS] database_name [COMMENT database_comment] [LOCATION hdfs_path\n[WITH DBPROPERTIES (property_name=property_value, ...)];\n3·\t简述静态分区和动态分区的关系\n有静态分区，可以没有动态分区，有动态分区，默认至少有一个静态分区\n1、聚合函数是一类比较特殊的函数。其可以对多行进行一些计算，然后得到一个结果值。更确切的说，这是用户自定义聚合函数，这类函数中最有名的两个例子是哪两个？分别用于什么。\n答：两个例子是count和avg。函数count用于计算有多少行数据（或者某列有多少值），而函数avg可以返回指定列的平均值。\n\n2、当进行算术运算时，需要注意什么问题？请详细说明。\n答：需要注意数据溢出或数据下溢问题。Hive遵循的底层Java中数据类型的规则，因此当溢出或下溢发生时计算结果不会自动转换为更广泛的数据类型。乘法和除法最有可能会引发这个问题。\n\n3、与聚合函数“相反的”一类函数是什么函数？它可以做什么？\n答：相反的一类函数是表生成函数，它可以将单列扩展成多列或者多行。\n\n4、SELECT语句用于什么？WHERE语句用于什么？两者结合使用又可以做什么？\n答：select语句用于选取字段，where语句用于过滤条件，两者结合使用可以查找到符合过滤条件的记录。\n\n5、GROUP BY语句通常会和聚合函数一起使用，怎么分组呢？\n答：按照一个或者多个列对结果进行分组，然后对每个组执行聚合操作。\n\n6、Hive可以在map端执行连接过程，Hive对于哪两个连接不支持这个优化。\n答：右外连接（RIGHT OUTER JOIN）和全外连接（FULL OUTER JOIN）\n\n7、UNION ALL可以将两个或多个表进行合并，每一个UNION子查询需要具有什么条件？\n答：每一个UNION子查询都必需具有相同的列，而且对应的每个字段的数据类型都必须是一致的。\n\n8、对于非常大的数据集，如何满足用户需要的是一个具有代表性的查询结果而不是全部结果？\n答：Hive可以通过对表进行分桶抽样来满足这个需求。\n\n9、如果所有表中的数据是分桶的，那么在特定的情况下对于大表同样可以使用map-side JOIN，请简单描述下应满足的条件。\n答：简单地说，表中的数据必须是按照ON语句中的键进行分桶的，而且其中一个表的分桶的个数必须是另一个表分桶个数的若干倍。当满足这些条件时，Hive可以在map阶段按照分桶数据进行连接。\n\n10、MapReduce job中传输的所有数据都是按照键值对的方式进行组织的，什么时候必须在内部使用这个功能？\n答：Hive在将用户的查询语句转换成MapReduce job时，其必须在内部使用这个功能。\n\n\n","tags":["答案"]},{"title":"软件工程","url":"/2020/06/24/软件工程/","content":"## 软件工程知识部分\n### 概述\n软件工程是指导计算机软件**开发**和**维护**的工程科学。\n软件工程是一门**综合性交叉**学科，计算机学着重于理论和科学 ， \n软件工程着重于**设计实现软件系统** 。\n### 软件的生命周期\n软件生命期分为7大阶段:  \n①　问题定义：要解决的问题是什么\n②　可行性研究：确定问题是否值得解，技术可行性、经济可行性、操作可行性\n③　需求分析：系统必须做什么 \n④　总体设计：系统如何实现，包括系统设计和结构设计 \n⑤　详细设计：具体实现设计的系统\n⑥　实现：编码和测试 \n⑦　运行维护：保证软件正常运行。\n软件生存周期的三大阶段：计划阶段、开发阶段、维护阶段。\n软件生命周期花费最多的阶段是**软件维护**。\n软件可行性研究包括：技术可行性、经济可行性、法律可行性。\n软件详细设计的主要任务是确定每个模块的算法和使用的数据结构。\n需求分析阶段产生的最重要的文档之一是需求规格说明书。\n### 数据流图和数据字典\n数据流图（DFD）中的每个数据处理至少需要一个输出数据流。\n数据流图中的箭头表示 数据流 ，椭圆或圆形表示 事务处理（数据处理） ，矩形表示 数据圆点点/终点。\n结构化程序设计的一种基本方法是**逐步求精法**。\n数据字典是软件需求分析阶段的最重要工具之一，其最基本的功能是数据定义。\n数据字典中有4类条目：数据流、数据项、 数据存储  、加工  \n数据字典是用来定 数据流图中的各个成分的具体含义的\n### 白箱测试技术\n白盒测试技术包括：.逻辑覆盖、基本路径测试、循环覆盖测试\n白盒测试技术测试用例的设计中，语句覆盖是最弱的覆盖标准。\n### 其他\n程序的三种基本结构是顺序、选择、循环\n耦合：软件结构中模块之间互相依赖的程度。\n（1）数据耦合\n（2）控制耦合\n（3）公共环境耦合\n（4）内容耦合\n## UML知识部分\n### UML体系\nUML体系包括三个部分：UML 基本构造块，UML规则 和 UML 公共机制。\n统一建模语言(UML)概念：统一建模语言(UML)是一种绘制软件蓝图的标准语言。可以用 UML 对软件密集型系统的制品进行可视化详述和文档化。UML 是一种定义良好、易于表达、功能强大且普遍适用的可视化建模语言。它融入了软件工程领域的新思想、新方法和新技术。它的作用域不限于支持面向对象的分析与设计，还支持从需求分析开始的软件开发的全过程。UML 的作用就是用很多图从静态和动态方面来全面描述我们将要开发的系统。\n面向对象的开发方法中,面向对象技术领域内占主导地位的标准建模语言是UML语言。\n面向对象方法的要素是对象、类、继承和消息。\n面向对象程序的基本特征是： **抽象** 、 **封装** 、 **继承** 和 **多态** \n在UML提供的图中，用例图用于描述系统与外部系统及用户之间的交互。\n用例图由参与者（Actor）、用例（Use Case）以及它们之间的关系构成的用于描述系统功能的图成为用例。执行者（Actor）与用例之间的关系是**关联关系**。\n\n用例图功能:\n（1）用例图是从软件需求分析到最终实现的第一步，它显示了系统的用户和用户希望提供的功能，有利于用户和软件开发人员之间的沟通。\n（2）用例图可视化的表达了系统的需求，具有直观、规范等优点，克服了纯文字性说明的不足.\n（3）用例方法是完全从外部来定义系统的，它把需求和设计完全分离开来，使用户不用关心系统内部是如何完成各种功能的 。\n\n在 UML 用例图中，椭圆表示 用例 ，方框表示 系统边界 ，小人状图案表示 执行者\t。\nUML 提供了一系列的图支持面向对象的分析与设计，其中类图给出系统的静态设计视图；用例图对系统的行为进行组织和建模是非常重要的；顺序图和协作图都是描述系统动态视图的交互图，其中顺序图描述了以时间顺序组织的对象之间的交互活动，协作图强调收发消息的对象的组织结构。\n顺序图包含4个元素，分别是对象、生命线、消息和激活\n面向对象中类与类之间的关系有四种:\n①　泛化关系:泛化是一种继承关系，表示一般与特殊的关系，它指定了子类如何特化父类的所有特征和行为.\n②　实现关系：用于规定规格说明与其实现之间的关系，换句话说，就是指定两个实体之间的一个合同，一个实体定义一个合同，而另一个实体保证履行该合同。\n③　关联关系：对象之间的关系准则。又分聚合关系和组合关系.\n④　依赖关系：当两个元素处于依赖关系中时，其中一个元素的改变可能会影响或提供消息给另一个元素，即另一个元素以某种形式依赖于另一个元素。\n\nUML状态图:描述一个对象的生命周期的.\nUML类图:用户根据用例图抽象成类，描述类的内部结构和类与类之间的关系，是一种静态结构图。 在UML类图中，常见的有以下几种关系: 泛化（Generalization）, 实现（Realization），关联（Association)，聚合（Aggregation），组合(Composition)，依赖(Dependency)。\n各种关系的强弱顺序： 泛化 = 实现 > 组合 > 聚合 > 关联 > 依赖\nUML活动图：第二代面向对象技术的标志。请熟悉活动图中各个模型元素。\nUML协作图特点:交互图的一种，描述了收发消息的对象的组织关系，强调对象之间的合作关系。时序图按照时间顺序布图，而写作图按照空间结构布图\nUML顺序图特点:交互图的一种，描述了对象之间消息发送的先后顺序，强调时间顺序。[详情链接](https://zhuanlan.zhihu.com/p/44518805)\n\nRUP 统一过程中的四个阶段：初始阶段、细化阶段、构造阶段、提交阶段。\nUML 中的交互图有两种，分别是顺序图和协作图，请分析一下两者之间的主要差别和各自的优缺点。掌握利用两种图进行的设计的方法。\n协作图可视化地表示了对象之间随时间发生的交互，它除了展示对象之间的关联，还显示出对象之间的消息传递。与顺序图一样，协作图也展示对象之间的交互关系。顺序图强调的是交互的时间顺序，而协作图强调的是交互的语境和参与交互的对象的整体组织。顺序图按照时间顺序布图，而协作图按照空间组织布图。\n顺序图可以清晰地表示消息之间的顺序和时间关系，但需要较多的水平方向的空间。\n协作图在增加对象时比较容易，而且分支也比较少，但如果消息比较多时难以表示消息之间的顺序。\n\n\n\n\n\n","tags":["答案"]},{"title":"数据结构第7章 图","url":"/2020/06/19/图/","content":"## 图的邻接矩阵表示法\n```\n#define MAXVUM 1000;\ntypedef char varTex;\ntypedef int ArcTex;\ntypdef struct{\n    varTex vexs[MAXVUM];\n    ArcTex arcs[MAXVUM][MAXVUM];\n    int vum,arcnum;\n}Graph;\n```\n使用邻接矩阵创建无向图\n```\n#define MaxInt 31242;\nStatus createUdm(Graph &G){\n    int i;\n    int j;\n    int k;\n    cin<<arcnum<<texnum\n    for(i=0;i<texnum;i++){\n        cin<<G.vex[i];\n    }\n    for(i=0;i<texnum;i++){\n    for(j=0;j<texnum;j++){\n        arcs[i][j]=MaxInt;\n    }\n    }\n    for(k=0;k<arcnum;k++){\n        cin<<v1<<v2<<w;\n        i=locate(v1);\n        j=locate(v2);\n        G.arcs[i][j]=w;\n        G.arcs[j][i]=G.arcs[i][j];\n    }\n}\n```\n## 图的邻接表存储方式\n```\ntypedef struct ArcNode{\nint v;\nstruct ArcNode* next;\nint info;\n}ArcNode;\ntypedef struct VNode{\n    int data;\n    arcnode* first;\n}VNode,adjlist[MVNum];\ntypedef struct{\nadjlist vertices;\nint vexnum,arcnum;\n}algraph;\n```","tags":["数据结构与算法"]},{"title":"数据结构第6章 树和二叉树","url":"/2020/06/18/树和二叉树/","content":"## 二叉树的性质\n性质1.二叉树的第i层至多有2的i-1次方个结点\n性质2.深度为k的二叉树至多有2的k次方减1个结点\n性质3.n0=n2+1\n性质4.具有n个结点的完全二叉树的深度为$[log_2n]+1$\n## 二叉树的链式存储\n```\ntypedef struct BTNode{\n    int data;\n    struct BTNode* leftchild,rightchild;\n}BTNode,*Bitree;\n```\n## 中序遍历的递归算法\n```\nvoid InOrderTraverse(Bitree T){\n    if(T){\n        InOrderTraverse(T->leftchild);\n        cout<<T->data;\n        InOrderTraverse(T->rightchild);\n    }\n}\n```\n## 中序遍历的非递归算法\n```\nvoid NoneOrderTraverse(Bitree T){\n    InitStack(S);\n    p=T;\n    q=new BiTNode;\n    while(p!=Null||isEmpty(S)){\n        if(p){\n            push(S,p);\n            p=p->leftchild;\n        }\n        else{\n            pop(S,q);\n            cout<<q->data;\n            p=q-rightchild;\n        }\n    }\n}\n```\n## 二叉树的二叉线索表示\n```\ntypedef struct BitrdNode{\nint data;\nint lTag;\nint rTag;\nBitrdTree *lchild,rchild;\n}BitrdNode,*BitrdTree;\n```\n## 以结点p为根的子树中序线索化\n```\nvoid InitTreading(BitrdTree T){\n    //pre是全局变量\n    if(p){\n        InitTreading(p->lchild);\n        if(!p->lchild){\n            p->lTag=1;\n            p->lchild=pre;\n        }\n        else p->lTag=0;\n        if(!pre->rchild){\n            pre->rTag=1;\n            pre->rchild=p;\n        }\n        else pre->rTag=0;\n        pre=p;\n        InitThreading(p->rchild);\n    }\n}\n```\n## 遍历中序搜索二叉树\n```\nvoid AllSearch(BitrdTree T){\n    p=T.lchild;\n    while(p){\n        while(p->lTag==0)p=p->lchild;\n        cout<<p->data;\n    while(p->rTag==1&&p!=T){\n        p=p->rchild;cout<<p->data;\n    }\n    p=p->rchild;\n    }   \n}\n```","tags":["数据结构与算法"]},{"title":"计算机网络第2章 物理层","url":"/2020/06/16/物理层/","content":"## 2.1.1物理层的基本概念\n物理层的主要任务描述为确定与传输媒体的接口有关的一些特性，即(1)机械特性(2)电器特性(3)功能特性(4)过程特性\n## 2.2.2有关信道的几个基本概念\n从通信双方的信息交互方式来看，可以有以下三种方式(1)单向通信(2)双向交替通信(3)双向同时通信。调制可以分两类，一类是仅仅对基带信号的波形进行变换。变换后信号仍然是基带信号。这类调制叫基带调制。另一类调制则需要使用载波进行调制，把基带信号的频率范围跳到较高的频段，并转换为模拟信号。经过载波调制后的信号称为带通信号，而使用载波的调制称为带通调制。\n常用的编码方式，不归零制，归零制，曼切斯特编码，差分曼切斯特编码。\n基本的带通调制方法，调频，调幅，调相。\n## 2.2.3信噪比和香农公式\n信噪比计算公式：信噪比=$10\\log_{10}(S/N)(dB)$  \n香农公式：$C=W\\log_2(1+S/N)(bit/s)$信道的极限传输速率为C\n##信道复用技术\n主要有频分复用，时分复用，统计时分复用，码分复用，波分复用","tags":["计算机网络"]},{"title":"数据结构第4章 串和模式匹配","url":"/2020/06/16/数据结构第5章 广义表/","content":"## 1.串的存储结构\n顺序存储结构\n```\n#define MAXSIZE 100\ntypedef struct{\n    char ch[MAXSIZE+1];\n    int length;\n}SString;\n```\n堆式存储结构\n```\ntypedef struct{\n    char ch*;\n    int length;\n}HString;\n```\n## 2.模式匹配算法\n### 2.1BF算法\n```\nint Index_BF(SString S,SString T,int pos){\n    int j=1,i=pos;\n    while(i<=S.length&&j<=T.length){\n        if(S.ch[i]==S.ch[j]){\n            ++i;++j;\n        }\n        else{\n            j=1;\n            i=i-j+2;\n        }\n    }\n    if(j>T.length) return i-T.length;\n    else return 0;\n}\n```\n### 2.2KMP算法\n```\nint Index_KMP(SString S,SString T,int pos{\nint j=1,i=pos;\nwhile(j<=T.length&&i<=S.length){\n    if(S.ch[i]==T.ch[j]){\n        ++i;\n        ++j;\n    }\n    else{\n        j=next[j];\n    }\n}\n  if(j>T.length) return i-T.length;\n    else return 0;\n}\n```\n计算next函数值\n```\nvoid get_next(SString T,int[] next){\n    int j=0;\n    int i=1;\n    next[1]=0\n    if(j==0||T.ch[i]==T.ch[j]){++i;++j;next[i]=j;};\n    else{\n        j=next[j];\n    }\n}\n```","tags":["数据结构与算法"]},{"title":"hive","url":"/2020/06/15/post/","content":"1、运算符~A数值类型，该如何描述？D\nA.A除以B。如果能整数除则返回商数 \nB.A和B按位取余\nC.A和B按位取与\nD.A按位取反\n2、查询语句中包含有（1-deductons[...]）这个运算，因为deductions字段是FLOAT类型的,因此数字1会提升为什么类型。C\nA.DOUBLE\nB.BIGINT\nC.FLOAT\nD.INT\n3、假设有一个表表示用户偏好，另一个表表示新闻文章，同时有一个算法会推测出用户可能会喜欢读哪些文章。这个时候就需要使用什么来生成所有用户和所有网页的对应关系的集合。B\nA.map-side JOIN\nB.笛卡尔积 JOIN\nC.RIGHT OUTER JOIN\nD.FULL OUTER JOIN\n4、哪个连接会返回左边表的记录，前提是其记录对于右边表满足ON语句中的判定条件。C\nA.笛卡尔积 JOIN\nB.RIGHT OUTER JOIN\nC.LEFT SEMI JOIN\nD.FULL OUTER JOIN\n5、Hive可以在map端执行连接过程，如果所有表中只有一个表是小表，那么可以在最大的表通过什么的时候将小表完全放到内存中。A\nA.mapper\nB.reduce\nC.select\nD.join\n6、DISTRIBUTE BY和GROUP BY在控制reducer如何接受一行行数据进行处理方面是类似的，而SORT BY则控制着什么的数据如何进行排序。A\nA.reduce内\nB.sort\nC.cluster\nD.以上都不是\n7、因为ORDER BY操作可能会导致运行时间过长，如果属性hive.mapred.mode的值是什么的话，那么Hive要求这样的语句必须加有LIMIT语句进行限制。B\nA.nonstrict\nB.strict\n8、SELECT和WHERE语句中不能引用什么表中的字段？B\nA.左边表\nB.右边表\n9、笛卡尔积是一种连接，表示左边表的行数怎么右边表的行数等于笛卡尔结果集的大小。D\nA.除以\nB.相加\nC.相减\nD.乘以\n10、左外连接通过关键字LEFT OUTER进行标识，在这种JOIN连接中，JOIN操作符左边表中符合WHERE子句的所有记录将会被返回。JOIN操作符右边表如果没有符合ON后面连接条件的记录时，那么从右边表制定选择的列的值将会是什么？C\nA.ALL\nB.NON\nC.NULL\nD.都不对\n二、判断题\n1、函数floor、round和ceil（向上取整）输入的是DOUBLE类型的值，而返回的值是BIGINT类型的，也就是将浮点型数转换成整型了。√\n2、返回值类型是DOUBLE，样式为degrees（DOUBLE d）将DOUBLE型角度值d转换成弧度值，结果是DOUBLE型的。×\n3、算数运算符接受任意的数值类型。√\n4、如果参与运算的两个数据的数据类型不同，那么两种类型中值的范围较小的那个数据类型将转换为值更大的数据类型。√\n5、用户不但可以选择则表中的列，还可以使用函数调用和算术表达式来操作列值。√\n6、对于INT类型和BIGINT类型计算，INT类型会转换提升为BIGINT类型；对于INT类型和FLOAT类型运算，INT类型将提升为FLOAT类型的。√\n7、CASE...WHEN...THEN语句和if条件语句类似，用于处理多个列的查询结果。×\n8、HAVING子句允许用户通过一个简单的语法完成原本需要通过子查询才能对FROUP BY语句产生的分组进行条件过滤的任务。√\n9、内连接中，不是只有进行连接的两个表中都存在与链接标准相匹配的数据才会被保留下来。×\n10、Hive可以在map端执行连接过程（称为map-side JOIN），这是因为Hive可以和内存中的小表进行逐一匹配，从而省略掉常规连接操作所需要的reduce过程。√[链接](https://blog.csdn.net/jinesse/article/details/80575991)\n三、解答题\n1、聚合函数是一类比较特殊的函数。其可以对多行进行一些计算，然后得到一个结果值。更确切的说，这是用户自定义聚合函数，这类函数中最有名的两个例子是哪两个？分别用于什么。\n聚合函数是一类比较特殊的函数，可以对多行进行一些计算，然后得到一个结果值，这类函数中最有名的两个例子就是count和avg，分别计算行数和平均值。[链接](https://www.jianshu.com/p/96b7b2b850bd)\n\n2、当进行算术运算时，需要注意什么问题？请详细说明。\n当进行算术运算时，用户需要注意数据溢出或数据下溢问题，Hive 遵循的是底层 Java 中数据类型的规则，因为当溢出或下溢发生时计算结果不会自动转换为更广泛的数据类型，乘法和除法最有可能会引发这个问题；[链接](https://www.cnblogs.com/jiuyi/p/4232452.html)\n\n3、与聚合函数“相反的”一类函数是什么函数？它可以做什么？\n与聚合函数“相反的”一类函数就是所谓的表生成函数，其可以将单列扩展成多列或者 多行。[链接](https://www.pianshen.com/article/1510470597/)\n\n4、SELECT语句用于什么？WHERE语句用于什么？两者结合使用又可以做什么？\n SELECT语句用于选取字段,WHERE语句用于过滤条件,两者结合使用可以查找到符合过滤条件的记录。\n\n5、GROUP BY语句通常会和聚合函数一起使用，怎么分组呢？\nGROUP BY语句通常会和聚合函数一起使用，按照一个或者多个列队结果进行分组，然后对每个执行聚合操作。[link](https://www.jianshu.com/p/ef4c9fb3bd5c)\n\n6、Hive可以在map端执行连接过程，Hive对于哪两个连接不支持这个优化。\nHive对于右外连接和全外连接不支持这个优化。\n\n7、UNION ALL可以将两个或多个表进行合并，每一个UNION子查询需要具有什么条件？\n只要结果集中的列数一致就可以.\n\n8、对于非常大的数据集，如何满足用户需要的是一个具有代表性的查询结果而不是全部结果？对于非常大的数据集，有时用户需要使用的是一个具有代表性的查询结果而不是全部结果。Hive可以通过对表进行抽样来满足这个需求。[link](https://blog.csdn.net/tyf2007635/article/details/98281835)\n\n9、如果所有表中的数据是分桶的，那么在特定的情况下对于大表同样可以使用map-side JOIN，请简单描述下应满足的条件。\n如果所有表中的数据是分桶的，那么对于大表，在特定的情况下同样可以使用这个优化，简单地说，表中的数据必须是按照ON语句中的键进行分桶的，而且其中一张表的分桶的个数必须是另一张表分桶个数的若干倍。[link](https://blog.csdn.net/xxydzyr/article/details/101012232)\n\n10、MapReduce job中传输的所有数据都是按照键值对的方式进行组织的，什么时候必须在内部使用这个功能？\nMapReduce job中传输的所有数据都是按照键值对的方式进行组织的，因此Hive在将用户的查询语句转换成MapReducejob时，其必须在内部使用这个功能。[link](https://blog.csdn.net/anzhouzan0567/article/details/101593144)\n","tags":["答案"]},{"title":"Java Stream Api","url":"/2020/03/18/Java-Stream-Api/","content":"\n不是数据结构;\n它没有内部存储，它只是用操作管道从source（数据结构、数组、generator function、IO channel）抓取数据;\n它也绝不修改自己所封装的底层数据结构的数据。例如Stream的filter操作会产生一个不包含被过滤元素的新Stream，而不是从source删除那些元素;\n所有Stream的操作必须以lambda表达式为参数;\n不支持索引访问;\n你可以请求第一个元素，但无法请求第二个，第三个，或最后一个;\n很容易生成数组或者List;\n惰性化;\n很多Stream操作是向后延迟的，一直到它弄清楚了最后需要多少数据才会开始;\nIntermediate操作永远是惰性化的;\n并行能力;\n当一个 Stream 是并行化的，就不需要再写多线程代码，所有对它的操作会自动并行进行的;\n可以是无限的。集合有固定大小，Stream 则不必。limit(n)和findFirst()这类的short-circuiting操作可以对无限的Stream进行运算并很快完成。\n","tags":["大数据技术"]},{"title":"古代中国性格迥异的帝王","url":"/2020/03/01/古代中国性格迥异的帝王/","content":"Sima Yan (236-290), the Emperor Wu of Jin, was the founder of Jin Dynasty. It was said that he had as much as 10,000 concubines. He once issued an imperial edict to “prohibit marriage” for years and carried out large-scale beauty pageants to select women for him, which increased his harem concubines by more than 5,000 beautiful women. He had so many concubines that it was difficult for him to decide which concubine to spend the night with. So he thought of a weird way, he sat in a cart drawn by sheep and let them walk randomly in the palace, he would spent night with the lucky girl inside the room where the sheep stopped.\n\nQianlong Emperor (1711 – 1799) of Qing Dynasty is familiar to Chinese people, but he also has another status – poet. He is the poet with the most poems in the world. According to records, he composed 41863 poems in his life. While the poem collection “Quan Tang Shi” (Complete Tang Poems), the largest collection of Tang poetry, had only 48,000 from more than 2200 authors. Qianlong`s life span was 89 years which was equivalent to around 32,000 days. Removing his childhood, he had less than 30,000 days to write poetry. It means that he created more than one poem per day.\n\nZhu Youtang (1470-1505), Emperor Xiaozong of Ming Dynasty, was the only emperor who had only one wife. He only married a empress surnamed Zhang and had no concubines. They lived together like living life of ordinary people. It was not easy to do so as an emperor. It is said that he also invented the world’s first toothbrush.","tags":["外文阅读搬运"]},{"title":"Python数据分析与挖掘(一) 拉格朗日插值法","url":"/2020/01/03/Python数据分析与挖掘-一/","content":"拉格朗日插值法：根据数学知识可知，对于平面上已知的n个点，可以找到一个n-1次多项式，使此多项式曲线过n个点，语言：Python\n```bash\n#拉格朗日插值代码\nimport pandas as pd #导入数据分析库Pandas\nfrom scipy.interpolate import lagrange #导入拉格朗日插值函数\n\ninputfile = '../chapter4/demo/data/catering_sale.xls' #销量数据路径\noutputfile = '../chapter4/demo/data/sales.xls' #输出数据路径\ndata = pd.read_excel(inputfile) #读入数据\ndata[u'销量'][(data[u'销量'] < 400) | (data[u'销量'] > 5000)] = None #过滤异常值，将其变为空值\nprint(data.columns)\n#自定义列向量插值函数\n#s为列向量，n为被插值的位置，k为取前后的数据个数，默认为5\ndef ployinterp_column(s, n, k=5):\n  y = s[list(range(n-k, n)) + list(range(n+1, n+1+k))]#取数\n  y = y[y.notnull()] #剔除空值,返回一个没有空值的列向量\n  return lagrange(y.index, list(y))(n) #插值并返回插值结果\n\n#逐个元素判断是否需要插值\nfor i in data.columns:\n  for j in range(len(data)):\n    if (data[i].isnull())[j]: #如果为空即插值。\n      data[i][j] = ployinterp_column(data[i], j)\n\ndata.to_excel(outputfile) #输出结果，写入文件\n```\nlangrange(x,y)(n)这个函数表示插入缺失值所在的下标n，并用插值多项式得到近似值，详情请使用文档查看，jupyter查看方法：移动光标到相应位置shift+tab。","tags":["大数据技术"]},{"title":"Scala辅导班（二）集和映射","url":"/2020/01/02/Scala辅导班（二）集和映射/","content":"## 集和映射\n集（set）：可变集和不可变集\n映射（Map）：可变映射和不可变映射\n可变与不可变分别继承于scala.collection.mutable._和scala.collection.immutable._\n测试的完整代码如下\n```bash\nobject mapAndSet {\n  def main(args:Array[String]){\n    var jetSet = Set(\"test\",\"exam\")//不可变集\n    val jetSet2 = mutable.Set(\"tiny2\",\"nico\")\n    jetSet+=\"tiny\"\n    jetSet2+=\"negopa\"\n    println(jetSet)\n    println(jetSet2)\n    val treasureMap = mutable.Map[Int,String]()\n    treasureMap+=(1 -> \"My\")\n    treasureMap+=(2 -> \"Name\")\n    treasureMap+=(3 -> \"is\")\n    println(treasureMap(2))\n    val rumerNum = Map(1 -> \"I\",2 -> \"II\",3 -> \"III\",4 -> \"IV\")//不可变映射\n    rumerNum.foreach(s => println(s._2))\n  }\n}\n```","tags":["语言学习基础"]},{"title":"Scala辅导班(一) 使用列表","url":"/2019/12/31/Scala辅导班（一）使用列表/","content":"新年快乐，2020，向前冲！\n语言：scala 2.12\n## 数组定义\n```bash\nval numNames = Array(\"num1\",\"num2\",\"num3\")\n```\n数组定义的一般形式\n```bash\nval numNames2 = Array.apply(\"num1\",\"num2\",\"num3\")\n```\n数组定义的等价形式，使用apply方法，scala语言的本质\n## 列表定义\n```bash\nval OneTwo = List(1,2)\nval ThreeFour = List(3,4)\n```\n列表定义\n```bash\nval OneTwoThreeFour = OneTwo ::: ThreeFour\n```\n列表衔接\n```bash\nval TwoThreeFour = 2::ThreeFour\n```\n向表头添加元素，为什么是向头部添加，因为向后添加会消耗线性时间，与其这样不如向表头添加，再通过反转，就能完成同样的功能\n## 列表常用方法\n```bash\nval thrill = \"nil\"::\"sekiro\"::\"thunder\"::Nil\nprint(thrill.count(s => s.length>=4))\nprint(thrill.filter(s => s.length>=4))\nprint(thrill.forall(s => s.endsWith(\"l\")))\nthrill.foreach(s => println(s))\nprint(thrill.mkString(\",\"))//返回以\",\"分割组成的一个列表\n```\n自己动手实践，不多讲","tags":["语言学习基础"]},{"title":"Spark学习（二）概念梳理","url":"/2019/12/29/Spark学习（二）概念梳理/","content":"\n伟大领袖，天降伟人的刘老师给我们的spark概念图\n{% img /img/Spark.png 800 450 %}\n## 分区\nSpark的分区是逻辑上的分区。执行过程中有多少个Task是由RDD分区决定的。同时运行几个Task是由分配给Executor的Core的数量决定的。\n## 什么时候分区\n·分区数量从少到多·处理能力不够·数据倾斜·需要合并场景\n## cache缓存\n持久化方法，将RDD放入内存中，减少重复计算,一共有12种\n    例：\n    ```bash\n    val MEMORY_AND_DISK = new StorageLevel(true, true, false, true)\n    ```\n//第一个参数，放到磁盘\n    //第二个参数，放到内存\n    //第三个参数，磁盘中的数据，不是以java对象的方式保存\n    //第四个参数，内存中的数据，以java对象的方式保存\n## checkpoint机制\n将RDD存储在本地磁盘或者HDFS中，通过直接读取检查点目录的数据来恢复相应的RDD\n## 宽依赖窄依赖\n    宽依赖：一个父RDD和一个或多个子RDD\n    窄依赖：一个子RDD和一个或多个父RDD\n## 广播\n将数据块缓存到所有节点\n## 启动流程(9step)\n    1、跟Master建连接并申请资源，每个Executor需要 3g 内存，一共4 cores\n    2、Master进行资源调度\n    3、Master跟Worker进行rpc通信，让Worker创建Executor\n    4、Worker启动Executor\n    5、Executor跟Driver进行通信\n    6、RDD触发Action后，会根据最后这个RDD从后往前推断依赖关系，遇到shufle就切分Stage，会递归切分，递归的出口是某个RDD没有父BDD了\n    7、DAGScheduler切分完Stage后，先提交前面的Stage,执行完后在提交后面的Stage，Stage会生产Task，一个Stage 会生产很多业务逻辑相同的Task，然后将以TaskSet的形式传递给TaskScheduler\n    8、 TaskScheduler将Task序列化，根据资源情况，发送Task给Executor\n    9、Executor接收到Task后，先将Task反序列化，然后将Task用一个实现了Runnable接口的实现类包装起来，然后将该包装类丢入到线程池，然后包装类的run方法就会被执行，进而调用Task的计算逻辑\n## DStream\nDstream为每一批次的数据生成一个RDD实例，就是一个RDD序列\n## 几何平均数\n{% img /img/jihe.png 150 100 %}\n## UDF用户自定义函数\n一对一的用户定义函数，例如\n```bash\nspark.sql(\"SELECT name,age,from_unixtime(create_time,'yyyy-MM-dd HH:mm:ss') FROM t_usr\").show\n```\n实现用户自定义函数\n```bash\nspark.udf.register(\"func_name\",方法体)\n```\n多进一出UDAF（多对一）\n","tags":["大数据技术"]},{"title":"Spark学习（一）Spark集群模式讲解","url":"/2019/12/28/Spark学习（一）Spark集群模式讲解/","content":"  小坚又被Spark教训了，下面重点讲解Spark的集群\n  下面详细列举了Spark目前支持的部署模式。\n· Local模式：在本地部署单个Spark服务\n· Standalone模式：Spark自带的任务调度模式（国内常用)\n· Yarn模式：Spark使用Hadoop的YARN组件来进行\n· Mesos模式：Spark使用Mesos平台进行资源与任务的调度\n· Kubernetes模式：自Spark2.3.x版本之后才开始支持Kubernetes","tags":["大数据技术"]},{"title":"数据结构与算法（五）科赫曲线","url":"/2019/12/21/数据结构与算法（五）科赫曲线/","content":"语言：python\n递归就是常说的套娃，\n在递归函数中直接或间接调用函数，就是递归\n科赫曲线是在无限的长度中圈定有限的面积，\n实现为三步：1.找到三等分点S,T 2.做等边三角形 3.再对线段进行调用\n```bash\n#科赫曲线\nimport turtle as t\ndef koth(size,n):\n    if n==0:\n        t.fd(size)\n    else:\n        for seta in [0,60,-120,60]:\n            t.left(seta)\n            koth(size/3,n-1)\nif __name__ == '__main__':\n    koth(200,3)\n```","tags":["数据结构与算法"]},{"title":"数据结构与算法（四）穷举搜索","url":"/2019/12/21/数据结构与算法（四）穷举搜索/","content":"运行环境：python 3.7 Dev-C++\n判断是否A中的任意元素都能能够组成整数m?\npython版:\n```bash\nA = [1,5,7,10,21]\nn = len(A)\ndef solve(i,m):\n    if m==0:\n        return True\n    elif i>=n:\n        return False\n    res = solve(i+1,m) or solve(i+1,m-A[i])\n    return res\nif __name__ == '__main__':\n        if(solve(0,8)):\n            print(\"yes\")\n        else:\n            print(\"no\")\n```\nc语言版:\n```bash\n#include<stdio.h>\nint n,A[50];\nint solve(int i,int m){\n\tif (m ==0)return 1;\n\tif (i>=n)return 0;\n\tint res = solve(i+1,m)||solve(i+1,m-A[i]);\n\treturn res; \n} \nint main(){\n\tint q,M,i;\n\tscanf(\"%d\",&n);\n\tfor(i=0;i<n;i++) scanf(\"%d\",&A[i]);\n\tscanf(\"%d\",&q);\n\tfor(i=0;i<q;i++){\n\t  scanf(\"%d\",&M);\n\t  if(solve(0,M)) printf(\"yes\\n\");\n\t  else printf(\"no\\n\"); \n}\nreturn 0;\n}\n```\n","tags":["数据结构与算法"]},{"title":"数据结构与算法（三）分治与递归","url":"/2019/12/21/数据结构与算法（三）分治与递归/","content":"运行环境：python 3.7\n将问题分解\n函数->递归函数->分治法\n本篇介绍递归和分治\n```bash\ndef factorial(n):\n    if n == 1:\n        return 1\n    else:\n        return n*factorial(n-1)\ndef findMaximum(A,l,r):\n    m = (l+r)//2\n    if l == r-1:\n        return A[l]\n    else:\n        u = findMaximum(A,l,m)\n        v = findMaximum(A,m,r)\n        x = max(u,v)\n    return x\nif __name__ == '__main__':\n    print(factorial(4))\n    A = [1,2,4,2,3,4,6]\n    print(findMaximum(A,0,7))#l到r（不包括r）\n```\n结果如下\n![](/img/分治与递归.png)\n","tags":["数据结构与算法"]},{"title":"数据结构与算法（二）散列法","url":"/2019/12/21/数据结构与算法（二）散列法/","content":"运行环境：Dev-C++\n题目要求：\n请实现一个能执行以下命令的简单“字典”\ninsert str:向字典中添加字符串\nfind str:当前字典中包含str时输出yes，不包含str时输出no\n主要思想：将字符转变成数字，并使用散列函数H(k) = (h1(k)+i*h2(k))mod m\nh1(k)=k mod m\nh2(k)=1+ k mod (m-1)\n代码如下\n```bash\n#include<stdio.h>\n#include<string.h>\n\n#define M 1046527\n#define NIL (-1)\n#define L 14\n\nchar H[M][L];\n\nint getChar(char ch){\n\tif(ch=='A')return 1;\n\telse if(ch=='C')return 2;\n\telse if(ch=='G')return 3;\n\telse if(ch=='T')return 4;\n\telse return 0;\n} \nlong long getKey(char str[]){\n\tlong long sum=0,p=1,i;\n\tfor(i=0;i<strlen(str);i++){\n\t\tsum+=p*(getChar(str[i]));\n\t\tp*=5;\n\t}\n\treturn sum; \n}\nint h1(int key){\n\treturn key%M;\n}\nint h2(int key){\n\treturn 1+(key%(M-1));\n}\nint find(char str[]){\n\tlong long key,i,h;\n\tkey=getKey(str);\n\tfor(i=0;;i++){\n\t\th=(h1(key)+i*h2(key))%M;\n\t\tif(strcmp(H[h],str)==0)return 1;\n\t\telse if(strlen(H[h])==0)return 0;\n\t}\n\treturn 0;\n}\nint insert(char str[]){\n\tlong long key,i,h;\n\tkey = getKey(str);\n\tfor(i=0;;i++){\n\t\th=(h1(key)+i*h2(key))%M;\n\t\tif(strcmp(H[h],str)==0)return 1;\n\t\telse if(strlen(H[h])==0){\n\t\t\tstrcpy(H[h],str);\n\t\t\treturn 0;\n\t}\n\t}\n\treturn 0;\n}\nint main(){\n\tint i,n,h;\n\tchar str[L],com[9];\n\tfor(i=0;i<M;i++)H[i][0]='\\0';\n\tscanf(\"%d\",&n);\n\tfor(i=0;i<n;i++){\n\t\tscanf(\"%s %s\",com,str);\n\t\t\n\t\tif(com[0]=='i'){\n\t\t\tinsert(str);\n\t\t}else{\n\t\t\tif(find(str)){\n\t\t\t\tprintf(\"yes\\n\");\n\t\t\t}else{\n\t\t\t\tprintf(\"no\\n\");\n\t\t\t}\n\t\t}\n\t}\n\treturn 0;\n}\n```\n结果如下：\n![](/img/散列法.png)\n\n\n","tags":["数据结构与算法"]},{"title":"数据结构与算法（一）二分查找","url":"/2019/12/21/数据结构与算法（一）二分查找/","content":"运行环境：Dev-C++\n题目要求：\n有一含n个整数的数列S，含q个不相同整数的T，限制：S为升序排列\n请找出既在S又在T的整数的个数？\n代码如下\n```bash\n#include<stdio.h>\nint A[10000],n;\nbinarySearch(int key){\n\tint left=0;\n\tint right=n;\n\tint mid;\n\twhile (left<right){\t\n\t\tmid=(left+right)/2;\n\t\tif(key==A[mid])return 1;\n\t\tif(key>A[mid])left=mid+1;\n\t\telse if(key<A[mid])right=mid;\n}\n\treturn 0;\n}\nint main(){\n\tint i,q,k,sum=0;\n\tscanf(\"%d\",&n);\n\tfor(i=0;i<n;i++){\n\t\tscanf(\"%d\",&A[i]);\n\t}\n\tscanf(\"%d\",&q);\n\tfor(i=0;i<q;i++){\n\t\tscanf(\"%d\",&k);\n\t\tif(binarySearch(k))sum++;\n\t}\n\tprintf(\"%d\\n\",sum);\n\treturn 0;\n} \n```\n结果如下：\n![](/img/二分查找.png)","tags":["数据结构与算法"]}]